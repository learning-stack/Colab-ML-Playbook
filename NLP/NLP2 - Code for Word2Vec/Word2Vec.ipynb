{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "id": "Fd9ZREqSWfMZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP- Code for Word2Vec neural network(Tensorflow)\n",
        "## by Madhu Sanjeevi"
      ]
    },
    {
      "metadata": {
        "id": "gdJHMkGQWq6J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://medium.com/deep-math-machine-learning-ai/chapter-9-2-nlp-code-for-word2vec-neural-network-tensorflow-544db99f5334"
      ]
    },
    {
      "metadata": {
        "id": "GU4ZtQC_Wx3Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/NLP/NLP2%20-%20Code%20for%20Word2Vec/Word2Vec.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/NLP2%20-%20Code%20for%20Word2Vec/Word2Vec.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "exRcZJigWfMd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import codecs\n",
        "\n",
        "!wget https://raw.githubusercontent.com/mneedham/neo4j-himym/master/data/import/sentences.csv\n",
        "\n",
        "input_file = codecs.open(\"sentences.csv\", \"r\",encoding='utf-8', errors='replace')\n",
        "\n",
        "data = pd.read_csv(input_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1_zPkZcPWfMi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9oMZCn2jWfMr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df= data.loc[data['Season']==1]\n",
        "df=df.fillna(\"(\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CmdRQMJYWfMu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['Character']=\"\"\n",
        "for sen,id in zip(df[\"Sentence\"],df[\"SentenceId\"]):\n",
        "    \n",
        "    if \"[\" is sen[0] or \"(\" is sen[0] or \"Scene\" in sen[0:5] or \":\" not in sen:\n",
        "        df = df.drop(df[df['Sentence']==sen].index)\n",
        "        \n",
        "    else:\n",
        "        value=\"\"\n",
        "        for c in sen:\n",
        "            if c==\":\":\n",
        "                df.set_value(id-1, 'Sentence', sen[len(value)+1:])\n",
        "                break\n",
        "                \n",
        "            else:\n",
        "                value+=c\n",
        "                \n",
        "        \n",
        "        if \"(\" in value:\n",
        "            index=value.index(\"(\")\n",
        "            value=value[:index-1]\n",
        "            \n",
        "        value=value.lower()    \n",
        "        #df.loc[df['Sentence'] == sen,'Character']=sen #setting the values\n",
        "        df.set_value(id-1, 'Character', value)\n",
        "\n",
        "#re arrange the order \n",
        "df = df[['SentenceId','EpisodeId','Season','Episode','Character','Sentence']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JQplgBJyWfMx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ocr_Ja2xWfM2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for labeling in plotting\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.text(rect.get_x() + rect.get_width()/2., 1.02*height,\n",
        "                '%d' % int(height),\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "topchar=df.Character.value_counts()[:7]\n",
        "labels=np.array(topchar.keys())\n",
        "values=np.array(topchar)\n",
        "ind=np.arange(len(labels))\n",
        "\n",
        "fig,ax=plt.subplots()\n",
        "rects=ax.bar(ind,values,color='g')\n",
        "ax.set_xticklabels(labels,rotation='vertical')\n",
        "ax.set_xticks(ind)\n",
        "ax.set_ylabel(\"Count\")\n",
        "ax.set_title(\"Top characters with most dialogs\")\n",
        "autolabel(rects)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HmGSsukIWfM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pre processing the text \n",
        "import re\n",
        "def normalize_text(text):\n",
        "\n",
        "    text=text.lower()\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(pic\\.twitter\\.com/[^\\s]+))','', text)\n",
        "    text = re.sub('@[^\\s]+','', text)\n",
        "    text = re.sub('#([^\\s]+)', '', text)\n",
        "    text = re.sub('[:;>?<=*+()&,\\-#!$%\\{˜|\\}\\[^_\\\\@\\]1234567890’‘]',' ', text)\n",
        "    text = re.sub('[\\d]','', text)\n",
        "    text = text.replace(\".\", '')\n",
        "    text = text.replace(\"'\", '')\n",
        "    text = text.replace(\"`\", '')\n",
        "    text = text.replace(\"'s\", '')\n",
        "    text = text.replace(\"/\", ' ')\n",
        "    text = text.replace(\"\\\"\", ' ')\n",
        "    text = text.replace(\"\\\\\", '')\n",
        "    re.sub(' +', ' ', text)\n",
        "    text=text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
        "    #normalize some utf8 encoding\n",
        "    text = text.replace(\"\\x9d\",'').replace(\"\\x8c\",'')\n",
        "    text = text.replace(\"\\xa0\",'')\n",
        "    text = text.replace(\"\\x9d\\x92\", '').replace(\"\\x9a\\xaa\\xf0\\x9f\\x94\\xb5\", '').replace(\"\\xf0\\x9f\\x91\\x8d\\x87\\xba\\xf0\\x9f\\x87\\xb8\", '').replace(\"\\x9f\",'').replace(\"\\x91\\x8d\",'')\n",
        "    text = text.replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\",'').replace(\"\\xf0\",'').replace('\\xf0x9f','').replace(\"\\x9f\\x91\\x8d\",'').replace(\"\\x87\\xba\\x87\\xb8\",'')\n",
        "    text = text.replace(\"\\xe2\\x80\\x94\",'').replace(\"\\x9d\\xa4\",'').replace(\"\\x96\\x91\",'').replace(\"\\xe1\\x91\\xac\\xc9\\x8c\\xce\\x90\\xc8\\xbb\\xef\\xbb\\x89\\xd4\\xbc\\xef\\xbb\\x89\\xc5\\xa0\\xc5\\xa0\\xc2\\xb8\",'')\n",
        "    text = text.replace(\"\\xe2\\x80\\x99s\", \"\").replace(\"\\xe2\\x80\\x98\", '').replace(\"\\xe2\\x80\\x99\", '').replace(\"\\xe2\\x80\\x9c\", \"\").replace(\"\\xe2\\x80\\x9d\", \"\")\n",
        "    text = text.replace(\"\\xe2\\x82\\xac\", \"\").replace(\"\\xc2\\xa3\", \"\").replace(\"\\xc2\\xa0\", \"\").replace(\"\\xc2\\xab\", \"\").replace(\"\\xf0\\x9f\\x94\\xb4\", \"\").replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\\xf0\\x9f\", \"\")\n",
        "    text =  re.sub(r\"\\b[a-z]\\b\", \"\", text)\n",
        "    text=re.sub( '\\s+', ' ', text).strip()\n",
        "    \n",
        "    text=re.sub(r'\\.+', \".\", text)\n",
        "    text=re.sub(r'\\.\\.+', ' ', text).replace('.', '')\n",
        "    # Replace multiple dots with space\n",
        "    text = re.sub('\\.\\.+', ' ', text) \n",
        "    # Remove single dots\n",
        "    text = re.sub('\\.', '', text)\n",
        "    text = re.sub(r'\\.{2,}', ' ', text)\n",
        "    text = re.sub(r'\\.{1}', '', text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ttOAqvcZflBG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So the sentence column is the actual raw data , we need to normalize the data ( removing symbols, spaces and etc…)"
      ]
    },
    {
      "metadata": {
        "id": "-nJ30GEoWfNE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentences=df[\"Sentence\"].tolist()\n",
        "normalized_sentences=[]\n",
        "for sentence in sentences:\n",
        "    norm_sent=normalize_text(sentence)\n",
        "    normalized_sentences.append(norm_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cwx3dDyqfvQf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "so now we have the clean data with us , let’s create a dictionary out of these sentences ( here I take unigrams (1 word) ) you can take bi-grams also,"
      ]
    },
    {
      "metadata": {
        "id": "VNNv9O6uWfNG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "# sentences to words and count\n",
        "words=\" \".join(normalized_sentences).split() \n",
        "count= collections.Counter(words).most_common() \n",
        "print (\"Word count\", count[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a_WDlYT8WfNK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build dictionaries\n",
        "unique_words =  [i[0] for i in count]\n",
        "dic = {w: i for i, w in enumerate(unique_words)} #dic, word -> id cats:0 dogs:1 ......\n",
        "voc_size = len(dic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7k8I44GCWfNO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "voc_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bs889Rxjf3Pv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "here we first identified the unique words by taking the count then we created the dictionary. ( word order depends based on the count).\n",
        "\n",
        "let me take the first sentence in our data set to see how it looks"
      ]
    },
    {
      "metadata": {
        "id": "PrYGkH6BWfNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make indexed word data\n",
        "data = [dic[word] for word in words] #count rank for every word in words\n",
        "print('Sample data', data[:10], words[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "saVvgFDsgHS_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We just replaced the words with numbers , remember these numbers are not the vectors , these are just the indexes in our dictionary.\n",
        "\n",
        "Now let’s create the word embeddings ( word2 vec)\n",
        "\n",
        "To build the model we can use skip gram ( as we discussed in the last story)\n",
        "\n",
        "Continuous Bag-of-Words model (CBOW)\n",
        "It predicts one word based on the surrounding words (it takes an entire context as an observation or takes a window sized context as an observation)\n",
        "\n",
        "Ex: Text= “Mady goes crazy about machine leaning” and window size is 3\n",
        "\n",
        "it takes 3 words at a time predicts the center word based on the surrounding words → [ [“Mady”,”crazy” ] , “goes”] → “goes” is the target word , and the other two are inputs.\n",
        "\n",
        "Skip-Gram model\n",
        "It takes one word as input and try to predict the surrounding (neighboring) words,\n",
        "\n",
        "[“Mady”, “goes”],[“goes”,”crazy”] → “goes” is the input word and “Mady” and “Crazy” are the surrounding words (Output probabilities)"
      ]
    },
    {
      "metadata": {
        "id": "4opbGoZPWfNV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's make a training data for window size 1 for simplicity\n",
        "\n",
        "cbow_pairs = []\n",
        "for i in range(1, len(data)-1) :\n",
        "    cbow_pairs.append([[data[i-1], data[i+1]], data[i]]);\n",
        "    \n",
        "print('Context pairs rank ids', cbow_pairs[:5])\n",
        "print()\n",
        "\n",
        "cbow_pairs_words = []\n",
        "for i in range(1, len(words)-1) :\n",
        "    cbow_pairs_words.append([[words[i-1], words[i+1]], words[i]]);\n",
        "print('Context pairs words', cbow_pairs_words[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lawE2kAwgTmG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "now we have the skip gram pairs as X and y values , lets create a functions that gives us the pairs batch wise"
      ]
    },
    {
      "metadata": {
        "id": "DCgCTwxoWfNa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's make skip-gram pairs\n",
        "# (quick, the), (quick, brown), (brown, quick), (brown, fox), ...\n",
        "skip_gram_pairs=[]\n",
        "\n",
        "for c in cbow_pairs:\n",
        "    skip_gram_pairs.append([c[1],c[0][0]])\n",
        "    skip_gram_pairs.append([c[1],c[0][1]])\n",
        "print('skip-gram pairs', skip_gram_pairs[:5])\n",
        "print()\n",
        "skip_gram_pairs_words=[]\n",
        "for c in cbow_pairs_words:\n",
        "    skip_gram_pairs_words.append([c[1],c[0][0]])\n",
        "    skip_gram_pairs_words.append([c[1],c[0][1]])\n",
        "print('skip-gram pairs words', skip_gram_pairs_words[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jiNUIkEAWfNh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batch(size):\n",
        "    assert size<len(skip_gram_pairs)\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    rdm = np.random.choice(range(len(skip_gram_pairs)),size,replace=False)\n",
        "    \n",
        "    for r in rdm:\n",
        "        X.append(skip_gram_pairs[r][0])\n",
        "        Y.append([skip_gram_pairs[r][1]])\n",
        "    return X , Y\n",
        "\n",
        "# generate_batch test\n",
        "print ('Batches (x, y)', get_batch(3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DYvyNDd_gZGX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we have the batch inputs to feed to Neural network so let’s build the neural network using tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "vu5F6qprWfNl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#tensor flow code\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 20\n",
        "embedding_size = 2\n",
        "num_sampled = 15    # Number of negative examples to sample.\n",
        "\n",
        "X= tf.placeholder(tf.int32,shape=[batch_size]) #inputs\n",
        "Y= tf.placeholder(tf.int32,shape=[batch_size,1]) #labels\n",
        "\n",
        "with tf.device(\"/cpu:0\"):\n",
        "      # Look up embeddings for inputs.\n",
        "    embeddings = tf.Variable(tf.random_uniform([voc_size,embedding_size],-1.0,1.0))\n",
        "    embed = tf.nn.embedding_lookup(embeddings, X) # lookup table\n",
        "    \n",
        "# Construct the variables for the NCE loss\n",
        "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size],-1.0, 1.0))\n",
        "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
        "\n",
        "# Compute the average NCE loss for the batch.\n",
        "# This does the magic:\n",
        "#   tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled, num_classes ...)\n",
        "# It automatically draws negative samples when we evaluate the loss.\n",
        "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases, Y, embed, num_sampled, voc_size))\n",
        "# Use the adam optimizer\n",
        "optimizer = tf.train.AdamOptimizer(1e-1).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W7xSN1UBge0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we dicussed in the last story , word2vec model has a 3 layer neural network (input , hidden and output).\n",
        "\n",
        "Here Hidden layer is just the dot product of inputs and weights ( no activation function here)\n",
        "\n",
        "Output layer has an activation function ( here it is Noise-contrastive estimation ) we can use softmax also here but NCE is good you can read about it in that paper(NCE internally uses the softmax).\n",
        "\n",
        "And we can define the size of hidden layer (here i took [voc_size, embedding_size(2)] but you can choose as your wish.\n",
        "\n",
        "okay let’s train the model for 10000 iterations"
      ]
    },
    {
      "metadata": {
        "id": "yK_XhblwWfNr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs=10000\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        batch_inputs, batch_labels = get_batch(batch_size)\n",
        "        _,loss_val=sess.run([optimizer,loss],feed_dict = {X : batch_inputs, Y : batch_labels })\n",
        "        \n",
        "        if epoch % 1000 == 0:\n",
        "            print(\"Loss at \", epoch, loss_val) # Report the loss\n",
        "    \n",
        "    # Final embeddings are ready for you to use. Need to normalize for practical use\n",
        "    trained_embeddings = embeddings.eval()\n",
        "    \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Show word2vec if dim is 2\n",
        "if trained_embeddings.shape[1] == 2:\n",
        "    labels = unique_words[:500] # Show top 500 words\n",
        "    for i, label in enumerate(labels):\n",
        "        x, y = trained_embeddings[i,:]\n",
        "        plt.scatter(x, y)\n",
        "        plt.annotate(label, xy=(x, y), xytext=(5, 2),\n",
        "            textcoords='offset points', ha='right', va='bottom')\n",
        "    #plt.savefig(\"word2vec.png\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j9z97lwZgkmW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So the error is decreased from 61 to 12 which is okay , now the model learned the weights so we got the final embeddings\n",
        "\n",
        "Let’s visualize them for top 50 words"
      ]
    },
    {
      "metadata": {
        "id": "NLizq5FfWfNz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Show word2vec if dim is 2\n",
        "if trained_embeddings.shape[1] == 2:\n",
        "    labels = unique_words[:50] # Show top 50 words\n",
        "    for i, label in enumerate(labels):\n",
        "        x, y = trained_embeddings[i,:]\n",
        "        plt.scatter(x, y)\n",
        "        plt.annotate(label, xy=(x, y), xytext=(5, 2),\n",
        "            textcoords='offset points', ha='right', va='bottom')\n",
        "    #plt.savefig(\"word2vec.png\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BsgXTfaeWfN4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}