{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mark_6.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "teq48pB7rikB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simple Approach to Multi-Label Classification\n",
        "## by Wilder Rodrigues"
      ]
    },
    {
      "metadata": {
        "id": "XaowpoPNrvFq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff"
      ]
    },
    {
      "metadata": {
        "id": "wwURPKchrxNy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/NLP/Deep%20dive%20into%20multi-label%20classification/Mark_6.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/Deep%20dive%20into%20multi-label%20classification/Mark_6.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "OTawWyKcrikE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. EDA"
      ]
    },
    {
      "metadata": {
        "id": "qSGbryQBrikE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fuqj1uOSrikI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "#printmd('**bold**')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p3Vfr0WerikL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_path = \"/Users/kartik/Desktop/AAIC/Projects/jigsaw-toxic-comment-classification-challenge/data/train.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4TrFTqXrrikP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_raw = pd.read_csv(data_path)\n",
        "#data_raw = data_raw.loc[np.random.choice(data_raw.index, size=2000)]\n",
        "data_raw.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rcgLbnpIrikV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Number of rows in data =\",data_raw.shape[0])\n",
        "print(\"Number of columns in data =\",data_raw.shape[1])\n",
        "print(\"\\n\")\n",
        "printmd(\"**Sample data:**\")\n",
        "data_raw.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "evc2LnvOrikZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1. Checking for missing values"
      ]
    },
    {
      "metadata": {
        "id": "byVKFaSfrika",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "missing_values_check = data_raw.isnull().sum()\n",
        "print(missing_values_check)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DyJW_1_yrikf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2. Calculating number of comments under each label"
      ]
    },
    {
      "metadata": {
        "id": "H8rmTDWJrikg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Comments with no label are considered to be clean comments.\n",
        "# Creating seperate column in dataframe to identify clean comments.\n",
        "\n",
        "# We use axis=1 to count row-wise and axis=0 to count column wise\n",
        "\n",
        "rowSums = data_raw.iloc[:,2:].sum(axis=1)\n",
        "clean_comments_count = (rowSums==0).sum(axis=0)\n",
        "\n",
        "print(\"Total number of comments = \",len(data_raw))\n",
        "print(\"Number of clean comments = \",clean_comments_count)\n",
        "print(\"Number of comments with labels =\",(len(data_raw)-clean_comments_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "J3CHgnVprikk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "categories = list(data_raw.columns.values)\n",
        "categories = categories[2:]\n",
        "print(categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bde9LIF4rikn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculating number of comments in each category\n",
        "\n",
        "counts = []\n",
        "for category in categories:\n",
        "    counts.append((category, data_raw[category].sum()))\n",
        "df_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])\n",
        "df_stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sDnOGcy4rikr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sns.set(font_scale = 2)\n",
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "ax= sns.barplot(categories, data_raw.iloc[:,2:].sum().values)\n",
        "\n",
        "plt.title(\"Comments in each category\", fontsize=24)\n",
        "plt.ylabel('Number of comments', fontsize=18)\n",
        "plt.xlabel('Comment Type ', fontsize=18)\n",
        "\n",
        "#adding the text labels\n",
        "rects = ax.patches\n",
        "labels = data_raw.iloc[:,2:].sum().values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kqx1Hgjtrikv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3. Calculating number of comments having multiple labels"
      ]
    },
    {
      "metadata": {
        "id": "2MT-2m3zrikw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rowSums = data_raw.iloc[:,2:].sum(axis=1)\n",
        "multiLabel_counts = rowSums.value_counts()\n",
        "multiLabel_counts = multiLabel_counts.iloc[1:]\n",
        "\n",
        "sns.set(font_scale = 2)\n",
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "ax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)\n",
        "\n",
        "plt.title(\"Comments having multiple labels \")\n",
        "plt.ylabel('Number of comments', fontsize=18)\n",
        "plt.xlabel('Number of labels', fontsize=18)\n",
        "\n",
        "#adding the text labels\n",
        "rects = ax.patches\n",
        "labels = multiLabel_counts.values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8uunYa-mrik0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4. WordCloud representation of most used words in each category of comments"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "sJxjKRFurik1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud,STOPWORDS\n",
        "\n",
        "plt.figure(figsize=(40,25))\n",
        "\n",
        "# toxic\n",
        "subset = data_raw[data_raw.toxic==1]\n",
        "text = subset.comment_text.values\n",
        "cloud_toxic = WordCloud(\n",
        "                          stopwords=STOPWORDS,\n",
        "                          background_color='black',\n",
        "                          collocations=False,\n",
        "                          width=2500,\n",
        "                          height=1800\n",
        "                         ).generate(\" \".join(text))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.axis('off')\n",
        "plt.title(\"Toxic\",fontsize=40)\n",
        "plt.imshow(cloud_toxic)\n",
        "\n",
        "\n",
        "# severe_toxic\n",
        "subset = data_raw[data_raw.severe_toxic==1]\n",
        "text = subset.comment_text.values\n",
        "cloud_severe_toxic = WordCloud(\n",
        "                          stopwords=STOPWORDS,\n",
        "                          background_color='black',\n",
        "                          collocations=False,\n",
        "                          width=2500,\n",
        "                          height=1800\n",
        "                         ).generate(\" \".join(text))\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.axis('off')\n",
        "plt.title(\"Severe Toxic\",fontsize=40)\n",
        "plt.imshow(cloud_severe_toxic)\n",
        "\n",
        "\n",
        "# obscene\n",
        "subset = data_raw[data_raw.obscene==1]\n",
        "text = subset.comment_text.values\n",
        "cloud_obscene = WordCloud(\n",
        "                          stopwords=STOPWORDS,\n",
        "                          background_color='black',\n",
        "                          collocations=False,\n",
        "                          width=2500,\n",
        "                          height=1800\n",
        "                         ).generate(\" \".join(text))\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.axis('off')\n",
        "plt.title(\"Obscene\",fontsize=40)\n",
        "plt.imshow(cloud_obscene)\n",
        "\n",
        "\n",
        "# threat\n",
        "subset = data_raw[data_raw.threat==1]\n",
        "text = subset.comment_text.values\n",
        "cloud_threat = WordCloud(\n",
        "                          stopwords=STOPWORDS,\n",
        "                          background_color='black',\n",
        "                          collocations=False,\n",
        "                          width=2500,\n",
        "                          height=1800\n",
        "                         ).generate(\" \".join(text))\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.axis('off')\n",
        "plt.title(\"Threat\",fontsize=40)\n",
        "plt.imshow(cloud_threat)\n",
        "\n",
        "\n",
        "# insult\n",
        "subset = data_raw[data_raw.insult==1]\n",
        "text = subset.comment_text.values\n",
        "cloud_insult = WordCloud(\n",
        "                          stopwords=STOPWORDS,\n",
        "                          background_color='black',\n",
        "                          collocations=False,\n",
        "                          width=2500,\n",
        "                          height=1800\n",
        "                         ).generate(\" \".join(text))\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.axis('off')\n",
        "plt.title(\"Insult\",fontsize=40)\n",
        "plt.imshow(cloud_insult)\n",
        "\n",
        "\n",
        "# identity_hate\n",
        "subset = data_raw[data_raw.identity_hate==1]\n",
        "text = subset.comment_text.values\n",
        "cloud_identity_hate = WordCloud(\n",
        "                          stopwords=STOPWORDS,\n",
        "                          background_color='black',\n",
        "                          collocations=False,\n",
        "                          width=2500,\n",
        "                          height=1800\n",
        "                         ).generate(\" \".join(text))\n",
        "\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.axis('off')\n",
        "plt.title(\"Identity Hate\",fontsize=40)\n",
        "plt.imshow(cloud_identity_hate)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HCJWjPM5rik7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Data Pre-Processing"
      ]
    },
    {
      "metadata": {
        "id": "R5m-3UTerik8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = data_raw\n",
        "data = data_raw.loc[np.random.choice(data_raw.index, size=2000)]\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nMZG-CezrilB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFjU6avjrilE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1. Cleaning Data"
      ]
    },
    {
      "metadata": {
        "id": "5gJk3JParilF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cleanHtml(sentence):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
        "    return cleantext\n",
        "\n",
        "\n",
        "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    cleaned = cleaned.strip()\n",
        "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def keepAlpha(sentence):\n",
        "    alpha_sent = \"\"\n",
        "    for word in sentence.split():\n",
        "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
        "        alpha_sent += alpha_word\n",
        "        alpha_sent += \" \"\n",
        "    alpha_sent = alpha_sent.strip()\n",
        "    return alpha_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4xR1br5YrilI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data['comment_text'] = data['comment_text'].str.lower()\n",
        "data['comment_text'] = data['comment_text'].apply(cleanHtml)\n",
        "data['comment_text'] = data['comment_text'].apply(cleanPunc)\n",
        "data['comment_text'] = data['comment_text'].apply(keepAlpha)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QvVOCYfBrilM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2. Removing Stop Words"
      ]
    },
    {
      "metadata": {
        "id": "6ZTL4IFHrilN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
        "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
        "def removeStopWords(sentence):\n",
        "    global re_stop_words\n",
        "    return re_stop_words.sub(\" \", sentence)\n",
        "\n",
        "data['comment_text'] = data['comment_text'].apply(removeStopWords)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B_GGvGJIrilR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3. Stemming"
      ]
    },
    {
      "metadata": {
        "id": "onoVqfWYrilS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer(\"english\")\n",
        "def stemming(sentence):\n",
        "    stemSentence = \"\"\n",
        "    for word in sentence.split():\n",
        "        stem = stemmer.stem(word)\n",
        "        stemSentence += stem\n",
        "        stemSentence += \" \"\n",
        "    stemSentence = stemSentence.strip()\n",
        "    return stemSentence\n",
        "\n",
        "data['comment_text'] = data['comment_text'].apply(stemming)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LzexSBLwrilW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.4. Train-Test Split"
      ]
    },
    {
      "metadata": {
        "id": "tbT4N06arilX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, random_state=42, test_size=0.30, shuffle=True)\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gl6ScV_zrild",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_text = train['comment_text']\n",
        "test_text = test['comment_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vL5oGm_vrilk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.5. TF-IDF"
      ]
    },
    {
      "metadata": {
        "id": "MTk6J8Egrill",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
        "vectorizer.fit(train_text)\n",
        "vectorizer.fit(test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o7XOR2pXrilp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = vectorizer.transform(train_text)\n",
        "y_train = train.drop(labels = ['id','comment_text'], axis=1)\n",
        "\n",
        "x_test = vectorizer.transform(test_text)\n",
        "y_test = test.drop(labels = ['id','comment_text'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oaHUTGPyrilu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Multi-Label Classification"
      ]
    },
    {
      "metadata": {
        "id": "H_gJU4Scrilv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1. Multiple Binary Classifications - (One Vs Rest Classifier)"
      ]
    },
    {
      "metadata": {
        "id": "R-o7_RiPrilx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UOhrhYd9rilz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Using pipeline for applying logistic regression and one vs rest classifier\n",
        "LogReg_pipeline = Pipeline([\n",
        "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
        "            ])\n",
        "\n",
        "for category in categories:\n",
        "    printmd('**Processing {} comments...**'.format(category))\n",
        "    \n",
        "    # Training logistic regression model on train data\n",
        "    LogReg_pipeline.fit(x_train, train[category])\n",
        "    \n",
        "    # calculating test accuracy\n",
        "    prediction = LogReg_pipeline.predict(x_test)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z2GUtKa3ril7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2. Multiple Binary Classifications - (Binary Relevance)"
      ]
    },
    {
      "metadata": {
        "id": "hpFW2M-xril8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# using binary relevance\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# initialize binary relevance multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "classifier = BinaryRelevance(GaussianNB())\n",
        "\n",
        "# train\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predict\n",
        "predictions = classifier.predict(x_test)\n",
        "\n",
        "# accuracy\n",
        "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ya1yzFierimA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3. Classifier Chains"
      ]
    },
    {
      "metadata": {
        "id": "DHZoQXQgrimB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "dWdyUT2jrimD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# initialize classifier chains multi-label classifier\n",
        "classifier = ClassifierChain(LogisticRegression())\n",
        "\n",
        "# Training logistic regression model on train data\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predict\n",
        "predictions = classifier.predict(x_test)\n",
        "\n",
        "# accuracy\n",
        "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aybqz76trimG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.4. Label Powerset"
      ]
    },
    {
      "metadata": {
        "id": "AX7K9CVHrimG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# using Label Powerset\n",
        "from skmultilearn.problem_transform import LabelPowerset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FonE1552rimI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# initialize label powerset multi-label classifier\n",
        "classifier = LabelPowerset(LogisticRegression())\n",
        "\n",
        "# train\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predict\n",
        "predictions = classifier.predict(x_test)\n",
        "\n",
        "# accuracy\n",
        "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PG97lIQOrimL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.5. Adapted Algorithm"
      ]
    },
    {
      "metadata": {
        "id": "MxgZdx7frimM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://scikit.ml/api/api/skmultilearn.adapt.html#skmultilearn.adapt.MLkNN\n",
        "\n",
        "from skmultilearn.adapt import MLkNN\n",
        "from scipy.sparse import csr_matrix, lil_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8OttKvAprimO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "classifier_new = MLkNN(k=10)\n",
        "\n",
        "# Note that this classifier can throw up errors when handling sparse matrices.\n",
        "\n",
        "x_train = lil_matrix(x_train).toarray()\n",
        "y_train = lil_matrix(y_train).toarray()\n",
        "x_test = lil_matrix(x_test).toarray()\n",
        "\n",
        "# train\n",
        "classifier_new.fit(x_train, y_train)\n",
        "\n",
        "# predict\n",
        "predictions_new = classifier_new.predict(x_test)\n",
        "\n",
        "# accuracy\n",
        "print(\"Accuracy = \",accuracy_score(y_test,predictions_new))\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8CrubsFCrimX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}