{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tag_Prediction.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "oAVVzBVqFDmU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Multi-Label Classification(Blog Tags Prediction)using NLP\n",
        "## by Indresh Bhattacharyya"
      ]
    },
    {
      "metadata": {
        "id": "OZWBw08AFPyS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://medium.com/coinmonks/multi-label-classification-blog-tags-prediction-using-nlp-b0b5ee6686fc"
      ]
    },
    {
      "metadata": {
        "id": "kFUOBTIHFRj7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/NLP/Multi-Label%20Classification%20Blog%20Tags%20Prediction/Tag_Prediction.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/Multi-Label%20Classification%20Blog%20Tags%20Prediction/Tag_Prediction.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "5ZavnXeVkckl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The Work Flow goes like this.**\n",
        "1. Scrape data from web\n",
        "2. Clean and Preprocess\n",
        "3. Visualize\n",
        "4. Classify\n",
        "\n",
        "**Objective:**\n",
        "\n",
        "In this project we are going to scrape data from medium and identify the tags given make a Data Frame out of it and in OneHotEncoding format and then.Classify which Blog post Fall under which tags."
      ]
    },
    {
      "metadata": {
        "id": "_6LoYJStF8Mi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import the needed libraries pandas for Data Frame and urllib3 for connecting to the web and fetching the data. Beautiful Soup is a Python package for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
      ]
    },
    {
      "metadata": {
        "id": "lZSubNMTFApc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib3\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YiXSrhwcFApj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "http=urllib3.PoolManager()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7WuuSX9bGFOK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating empty dataFrame for title and body **column=[‘Title’,’Body’] dfBA=DataFrame(columns=column)**\n",
        "\n",
        "Also one more for the tags dfT=DataFrame(columns=[0,1,2,3,4])"
      ]
    },
    {
      "metadata": {
        "id": "DIwhIY9AFApm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pandas import DataFrame\n",
        "column=['Title','Body']\n",
        "dfBA=DataFrame(columns=column)\n",
        "dfT=DataFrame(columns=[0,1,2,3,4])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zgxPCqTMlrmd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CreateDataFrame()** creates the dataframe for **dfDA** and **dfT** respectively for (Title and Body) and (Tags)"
      ]
    },
    {
      "metadata": {
        "id": "w9irMBB6llh0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def CreateDataFrame(someList):\n",
        "    t={}\n",
        "    d={'Title':[someList[0]],'Body':[someList[1]]}\n",
        "    for n in range(5):\n",
        "        if len(someList[2])>n:\n",
        "            t[n]=[someList[2][n]]\n",
        "        else:\n",
        "            t[n]=['0']\n",
        "    toDf=DataFrame(data=d)\n",
        "    global dfBA,dfT\n",
        "    #print(dfBA)\n",
        "    dfBA=dfBA.append(toDf)\n",
        "    #print(dfBA)\n",
        "    dfT=dfT.append(DataFrame(data=t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vJAcQqyPGM6K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CrawlAndFrame()** function in going into every link collected by the spider and from there it collects all the blog **article** and **title** of the blog with the tags involved in the blog and passes them **CreateDataFrame(someList)** which creates a dataframe for the head and article(**dfBA**) and another for the tags(dfT).\n",
        "\n",
        "Note: As urllib3 is not good at dynamic scraping it will only fetch 7 posts per page. Use selenium for dynamic scraping"
      ]
    },
    {
      "metadata": {
        "id": "Vq-FelkAFApo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def CrawlAndFrame(link):\n",
        "    print(link)\n",
        "    blogData=http.request('GET',link)\n",
        "    soup=BeautifulSoup(blogData.data,'html.parser')\n",
        "    article=''\n",
        "    tags=[]\n",
        "    heading=soup.find('h1').text\n",
        "    for para in soup.find_all('p'):\n",
        "        p=para.text\n",
        "        #p=p.strip('\\u')\n",
        "        article=article+' '+p\n",
        "    for mtags in soup.find_all('a',{'class':'link u-baseColor--link'}):\n",
        "        tags.append(mtags.text)\n",
        "    #CreateDataFrame(list())\n",
        "    someList=[heading,article,tuple(tags)]\n",
        "    #print(someList[0])\n",
        "    CreateDataFrame(someList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n64hOD0IGWZK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The spider function will go into the web pages and get the links for all the posts in the web page."
      ]
    },
    {
      "metadata": {
        "id": "9bCM44vDFApt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def spider(link):\n",
        "    blogData=http.request('GET',link)\n",
        "    soup=BeautifulSoup(blogData.data,'html.parser')\n",
        "    for links in soup.find_all('div',{'class':'postArticle-readMore'}):\n",
        "        link=links.find('a').get('href')\n",
        "        CrawlAndFrame(link)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bwcpr5PHFApx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url=['https://medium.com/search?q=machine%20learning','https://medium.com/search?q=deep%20learning']\n",
        "for x in url:\n",
        "    spider(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SPzE-_ujFAp3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QdlG53MzFAp5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_YzGTZ64FAp8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df=pd.concat([dfBA,dfT],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YwnAtilwFAp_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "67xAF9bDFAqC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopWordList=stopwords.words('english')\n",
        "stopWordList.remove('no')\n",
        "stopWordList.remove('not')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "job-ZGVXm4Cc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This removes every html tags. If there are any. Sometimes even after scraping some tags remain. We are removing that."
      ]
    },
    {
      "metadata": {
        "id": "BuxNS-6ymrkc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def removeTags(data):\n",
        "    soup=BeautifulSoup(data,'html.parser')\n",
        "    text=soup.get_text()\n",
        "    return text\n",
        "import unicodedata\n",
        "import spacy\n",
        "#nlp = spacy.load('en_core', parse=True, tag=True, entity=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LIMBtq6bm-P1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This function transforms all the accented characters into normal English .\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*OKud8hfMHa_z0Vxa5jZ3rA.jpeg)"
      ]
    },
    {
      "metadata": {
        "id": "D5VPIG1xmy2M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def removeAscendingChar(data):\n",
        "    data=unicodedata.normalize('NFKD', data).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_MruKlSnJNM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Removes all special characters and digits"
      ]
    },
    {
      "metadata": {
        "id": "bNK6sYsMm1yk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def removeCharDigit(text):\n",
        "    str='`1234567890-=~@#$%^&*()_+[!{;\":\\'><.,/?\"}]'\n",
        "    for w in text:\n",
        "        if w in str:\n",
        "            text=text.replace(w,'')\n",
        "    return text\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "lemma=WordNetLemmatizer()\n",
        "token=ToktokTokenizer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hx__9MJrnkp8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
        "\n",
        "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
        "\n",
        "am, are, is →be \n",
        "\n",
        "car, cars, car’s, cars →car"
      ]
    },
    {
      "metadata": {
        "id": "JrLX4BuTnd4E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lemitizeWords(text):\n",
        "    words=token.tokenize(text)\n",
        "    listLemma=[]\n",
        "    for w in words:\n",
        "        x=lemma.lemmatize(w,'v')\n",
        "        #print(x)\n",
        "        listLemma.append(x)\n",
        "    return text\n",
        "def stopWordsRemove(text):\n",
        "    \n",
        "    wordList=[x.lower().strip() for x in token.tokenize(text)]\n",
        "    \n",
        "    removedList=[x for x in wordList if not x in stopWordList]\n",
        "    text=' '.join(removedList)\n",
        "    #print(text)\n",
        "    return text\n",
        "def PreProcessing(text):\n",
        "    text=removeTags(text)\n",
        "    #print(text)\n",
        "    text=removeCharDigit(text)\n",
        "    #print(text)\n",
        "    text=removeAscendingChar(text)\n",
        "    #print(text)\n",
        "    text=lemitizeWords(text)\n",
        "    #print(text)\n",
        "    text=stopWordsRemove(text)\n",
        "    #print(text)\n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_twYFiGnoTR0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets visualize:"
      ]
    },
    {
      "metadata": {
        "id": "G0TeyYSKFAqF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "totalText=''\n",
        "for x in df['Body']:\n",
        "    ps=PreProcessing(x)\n",
        "    totalText=totalText+\" \"+ps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kZ-48bsTFAqH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "wc=WordCloud(max_font_size=60).generate(totalText)\n",
        "plt.figure(figsize=(16,12))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mOq7kGaVH_w7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What about frequency of the words?"
      ]
    },
    {
      "metadata": {
        "id": "WD3BD5dfFAqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "freqdist = nltk.FreqDist(token.tokenize(totalText))\n",
        "freqdist\n",
        "plt.figure(figsize=(16,5))\n",
        "freqdist.plot(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ncx9R-2KIEwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see the most frequent words in the article is Learning,Data,Machine,ai etc"
      ]
    },
    {
      "metadata": {
        "id": "5mwY53WBFAqP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "totalText=''\n",
        "for x in df['Title']:\n",
        "    ps=PreProcessing(x)\n",
        "    totalText=totalText+\" \"+ps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9TblNXDFAqS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "wc=WordCloud(max_font_size=60).generate(totalText)\n",
        "plt.figure(figsize=(16,12))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6J3JCWgrFAqa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "freqdist = nltk.FreqDist(token.tokenize(totalText))\n",
        "freqdist\n",
        "plt.figure(figsize=(16,5))\n",
        "freqdist.plot(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2IHFeJFFAqg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x=df.iloc[:,0:2].values\n",
        "y=df.iloc[:,2:-1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3MNCS2-4l5qd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looking at **dfT** You can probably tell that this is not a optimal way to the tag data. So we need to change it into OneHotEncoding format(**which is basically creating a sparse matrix of 0 and 1 where 1 represents that the index tag is present 0 represents that it doesnt **)"
      ]
    },
    {
      "metadata": {
        "id": "uM6USXQKFAql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "okList=[]\n",
        "for cl in dfT.columns:\n",
        "     for n in df[cl]:\n",
        "            okList.append(n)\n",
        "okList=list(set(okList))\n",
        "del(okList[okList.index('0')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gt9vRUkXFAqn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "okList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qdDdD800FAqr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "newDF=DataFrame(columns=okList)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E57b96HaFAqy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for x in range(dfT.count()[0]):\n",
        "    someDict={}\n",
        "    for d in okList:\n",
        "        rowdata=list(dfT.iloc[x])\n",
        "        if d in rowdata:\n",
        "            someDict[d]=1\n",
        "        else:\n",
        "            someDict[d]=0\n",
        "    newDF=newDF.append(someDict,ignore_index=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hPNW6OMHmIcF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In a nutshell what I did here was just taking all the unique tags in the List and made them the columns of my dataFrame and put 1 if that tag is present in the row and 0 if not"
      ]
    },
    {
      "metadata": {
        "id": "ecdlFbR9FAq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "newDF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-MNeJ8CiFAq5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dfBA.index=range(dfBA.count()[0])\n",
        "df=dfBA.join(newDF)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ya19z58DFAq7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ipm0hPpVFAq-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x=df.iloc[:,0:2].values\n",
        "y=df.iloc[:,2:-1].values\n",
        "# using binary relevance\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from pandas import DataFrame\n",
        "x1=df.Title\n",
        "x2=df.Body\n",
        "from pandas import DataFrame\n",
        "cv=CountVectorizer().fit(x1)\n",
        "header=DataFrame(cv.transform(x1).todense(),columns=cv.get_feature_names())\n",
        "cvArticle=CountVectorizer().fit(x2)\n",
        "article=DataFrame(cvArticle.transform(x2).todense(),columns=cvArticle.get_feature_names())\n",
        "import pandas as pd\n",
        "x=pd.concat([header,article],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WLgDku8JFArB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidfhead=TfidfTransformer().fit(header)\n",
        "head=DataFrame(tfidfhead.transform(header).todense())\n",
        "tfidfart=TfidfTransformer().fit(article)\n",
        "art=DataFrame(tfidfart.transform(article).todense())\n",
        "import pandas as pd\n",
        "x=pd.concat([head,art],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k1TfkP9Ro9WV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Classification(Training and Testing the model):**\n",
        "\n",
        "What is binary relevance??\n",
        "\n",
        "This is the simplest technique, which basically treats each label as a separate single class classification problem.\n",
        "\n",
        "Say we have x as a independent variable and y1,y2,y3 as the labels of dependent variable . So what binary relevance does is that is it treats each independent variable as a separate class in consideration to the independent variable.\n",
        "\n",
        "so it maps\n",
        "\n",
        "x →y1 and x →y2 and x →y3"
      ]
    },
    {
      "metadata": {
        "id": "OcsC5L-lFArE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install scikit-multilearn\n",
        "\n",
        "# using binary relevance\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "# initialize binary relevance multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "xtrain,xtest,ytrain,ytest=train_test_split(x,y)\n",
        "classifier = BinaryRelevance(GaussianNB())\n",
        "\n",
        "# train\n",
        "classifier.fit(xtrain.astype(float), ytrain.astype(float))\n",
        "\n",
        "predictions = classifier.predict(xtest.astype(float))\n",
        "predictions.toarray()\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(ytest.astype(float),predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VXwrUsDZpWBL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**What is classifier chains??**\n",
        "\n",
        "In this, the first classifier is trained just on the input data and then each next classifier is trained on the input space and all the previous classifiers in the chain.\n",
        "\n",
        "Say we have x as a independent variable and y1,y2,y3 as the labels of dependent variable . So we basically have 3 subsets of classification\n",
        "\n",
        "x →y1 and x →y1,y2 and x →y1,y2,y3"
      ]
    },
    {
      "metadata": {
        "id": "gNWc3ZcCFArJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# initialize classifier chains multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "classifier = ClassifierChain(GaussianNB())\n",
        "\n",
        "# train\n",
        "classifier.fit(xtrain.astype(float), ytrain.astype(float))\n",
        "\n",
        "# predict\n",
        "predictions = classifier.predict(xtest.astype(float))\n",
        "\n",
        "accuracy_score(ytest.astype(float),predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hgw3ctbIpuW0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**What is Label Powerset??**\n",
        "\n",
        "In this, we transform the problem into a multi-class problem with one multi-class classifier is trained on all unique label combinations found in the training data."
      ]
    },
    {
      "metadata": {
        "id": "_OpnKVWGFArM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# using Label Powerset\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# initialize Label Powerset multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "classifier = LabelPowerset(GaussianNB())\n",
        "\n",
        "# train\n",
        "classifier.fit(xtrain.astype(float), ytrain.astype(float))\n",
        "# predict\n",
        "predictions = classifier.predict(xtest.astype(float))\n",
        "\n",
        "accuracy_score(ytest.astype(float),predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OGYombXsp18M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**MLkNN:**\n",
        "\n",
        "Adapted algorithm, as the name suggests, adapting the algorithm to directly perform multi-label classification, rather than transforming the problem into different subsets of problems.\n",
        "\n",
        "For example, multi-label version of kNN is represented by MLkNN."
      ]
    },
    {
      "metadata": {
        "id": "fDITWIlHFArR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from skmultilearn.adapt import MLkNN\n",
        "\n",
        "classifier = MLkNN(k=7)\n",
        "\n",
        "# train\n",
        "classifier.fit(xtrain.astype(float), ytrain.astype(float))\n",
        "# predict\n",
        "predictions = classifier.predict(xtest.astype(float))\n",
        "\n",
        "accuracy_score(ytest.astype(float),predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2GEEJaXp8lM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "classifier=OneVsRestClassifier(MultinomialNB())\n",
        "\n",
        "# train\n",
        "classifier.fit(xtrain,ytrain)\n",
        "# predict\n",
        "predictions=classifier.predict(xtest)\n",
        "\n",
        "accuracy_score(ytrain,predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nbKXm4FZFArW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}