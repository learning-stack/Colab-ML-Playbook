{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message Classifier, Multi Class, Single Label\n",
    "Gilbert François Duivesteijn (gilbert@deep-impact.ch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dt010612.gif\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from lib.utils import plot_confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from sklearn import neighbors\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, linewidth=100)\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "\n",
    "def stemmed(doc):\n",
    "    return [stemmer.stem(w) for w in analyzer(doc)]\n",
    "\n",
    "\n",
    "def no_stemmed(doc):\n",
    "    return [w for w in analyzer(doc)]\n",
    "\n",
    "\n",
    "def mask_integers(s):\n",
    "    return re.sub(r'\\d+', '_INT', s)\n",
    "\n",
    "\n",
    "def mask_times(text):\n",
    "    \"\"\"\n",
    "    Replaces times written like 12:50, 1PM, 4:15am, etc to _time token.\n",
    "    :param    text    Input text\n",
    "    :return           Output text with replaced times.\n",
    "    \"\"\"\n",
    "    re_time1 = '\\d{1,2}[:.]\\d{2}(?:am|pm|AM|PM)'\n",
    "    re_time2 = '\\d{1,2}[:.]\\d{2}'\n",
    "    re_time3 = '\\d{1,2}(?:am|pm|AM|PM)'\n",
    "    rec_time = re.compile(re_time1 + '|' + re_time2 + '|' + re_time3)\n",
    "    return re.sub(rec_time, '_TIME', text)\n",
    "\n",
    "\n",
    "def mask_emojis(text):\n",
    "    \"\"\"\n",
    "    Replaces all different emojis to _emoji token.\n",
    "    :param    text    Input text\n",
    "    :return           Output text with replaced emojis.    \n",
    "    \"\"\"\n",
    "    re_icons = ':[a-z-_]*:'\n",
    "    re_ldsd = '\\<(.*?)\\>'\n",
    "    rec_icons = re.compile(re_icons + \"|\" + re_ldsd)\n",
    "    return re.sub(rec_icons, '_EMOJI', text)\n",
    "\n",
    "\n",
    "def mask_all(text):\n",
    "    text = mask_times(text)\n",
    "    text = mask_emojis(text)\n",
    "    text = mask_integers(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def train_and_test(steps, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Trains and tests the pipeline with the given steps. \n",
    "    :param steps:       List of operations inside the pipeline.\n",
    "    :param X_train:     Training data\n",
    "    :param X_test:      Training labels\n",
    "    :param y_train:     Testing data\n",
    "    :param y_test:      Testing labels\n",
    "    :return:            Trained model\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline(steps)\n",
    "    folds = 10\n",
    "    xval_score = cross_val_score(pipeline, X_train, y_train, cv=folds, n_jobs=-1)\n",
    "    \n",
    "    xv_min = np.min(xval_score)\n",
    "    xv_max = np.max(xval_score)\n",
    "    xv_mean = np.mean(xval_score)\n",
    "    xv_std = np.std(xval_score)\n",
    "    print('{} fold Cross Validation Score: <{:.2f}, {:.2f}>; µ={:.2f}'.format(folds, xv_min, xv_max, xv_mean))\n",
    "    pipeline = pipeline.fit(X_train, y_train)\n",
    "    print('Score on test set: {:.2f}'.format(pipeline.score(X_test, y_test)))\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def tag_message(pipeline, message):\n",
    "    y_pred = pipeline.predict([message])[0]\n",
    "    print('{:>20} | {}'.format(dict_classes[y_pred], message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges:\n",
    "- Real data, not easy separable in different classes.\n",
    "- Multi class classification, more difficult than binary classification.\n",
    "- Small dataset, not a lot of samples to train and test.\n",
    "- Number of samples are not equally divided over the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Class | Description                                       |\n",
    "| ----- | ------------------------------------------------- |\n",
    "| 1     | Too late, away during office hours or early leave |\n",
    "| 2     | Holidays or scheduled free days                   |\n",
    "| 3     | Home Office                                       |\n",
    "| 4     | Medical appointment                               |\n",
    "| 5     | Ill, without consulting a medical                 |\n",
    "| 6     | Work related absence (at client, conference)      |\n",
    "| 7     | In office announcement                            |\n",
    "| 8     | Miscellanious                                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classes = {\n",
    "    1: 'late/early',\n",
    "    2: 'holidays',\n",
    "    3: 'home office',\n",
    "    4: 'med app',\n",
    "    5: 'ill',\n",
    "    6: 'business',\n",
    "    7: 'in office',\n",
    "    8: 'miscellanious'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Model (VSM)\n",
    "\n",
    "### Vectorizer\n",
    "\n",
    "Before we can use the text messages to train a classifier, we have to transform text into numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"But I don’t want to go among mad people, Alice remarked.\",\n",
    "    \"Oh, you can’t help that, said the Cat: we’re all mad here. I’m mad. You’re mad.\",\n",
    "    \"How do you know I’m mad? said Alice.\",\n",
    "    \"You must be, said the Cat, or you wouldn’t have come here.\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "# vectorizer = CountVectorizer(tokenizer=nltk.tokenize.word_tokenize, stop_words='english', strip_accents='unicode')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It creates vector for every document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When transforming a text with unknown words, like `cat` in this example, vector is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents2 = [\"Cat: Where are you going?\",\n",
    "            \"Alice: Which way should I go?\"]\n",
    "X12 = vectorizer.transform(documents2).toarray()\n",
    "X12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermezzo: Search engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document collection\n",
    "D = X.toarray()\n",
    "\n",
    "# query\n",
    "q = vectorizer.transform([\"is alice mad?\"]).toarray()\n",
    "\n",
    "# Do the search by computing the dot product\n",
    "res = D * q\n",
    "\n",
    "# Sort highest ranked documents and show only documents with a score > 0\n",
    "res_ranked = np.sum(res, axis=1)\n",
    "res_index_sorted = np.argsort(res_ranked)[::-1]\n",
    "res_index_sorted_filtered = res_index_sorted[res_ranked[res_index_sorted] > 0]\n",
    "\n",
    "# Print the search results\n",
    "for index in res_index_sorted_filtered:\n",
    "    print('[ score: {} ] {}'.format(res_ranked[index], documents[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "With a count vectorizer, every word gets counted with the same weight, making frequently occuring words too important. There are several ways to penalizing frequent occuring words and rewarding rare occuring words. The most well known method is called term frequency - inverse document frequency (TF-IDF). TfIdf is implemented in scikit-learn as `TfidfTransformer`. Another algoritm is Okapi-BM25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "Xt = transformer.fit_transform(X)\n",
    "Xt.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing common words from the documents can improve the performance of the classifier. Both NLTK as CountVectorizer provide a lists of stopwords in different languages. Beware that the lists are not the same and might give different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sorted(nltk.corpus.stopwords.words('english')):\n",
    "    print('{}, '.format(word), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sorted(vectorizer.get_stop_words()):\n",
    "    print('{}, '.format(word), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming truncates variations of words into a same shape which helps the classifier to recognise these words as the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc11 = 'computer computers computing computed'\n",
    "print(no_stemmed(doc11))\n",
    "print(stemmed(doc11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cls_messages = pd.read_pickle('data/messages-cls.pkl')\n",
    "df_cls_messages.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows which have no annotation\n",
    "samples = df_cls_messages.dropna()\n",
    "\n",
    "# Convert the classification column to unsigned int, in case it is stored as string\n",
    "samples['class'] = samples.loc[:, 'class'].astype(np.uint8).values\n",
    "\n",
    "X = samples['text']\n",
    "y = samples['class']\n",
    "\n",
    "print('[.] Number of training samples: {}'.format(len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a pipeline and training the first model\n",
    "\n",
    "Scikit Learn offers a great way to combine the preprocessing (vectorization, stemming, stopword removal, etc) and training/predicting by building a pipeline. Let's see how that works...\n",
    "\n",
    "More information on pipelines, look at this nice blog post:\n",
    "\n",
    "https://buhrmann.github.io/sklearn-pipelines.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('vectorizer', CountVectorizer()),\n",
    "         ('classifier', LinearSVC(random_state=1))]\n",
    "pipeline1 = train_and_test(steps, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline1.predict(X_test)\n",
    "cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=dict_classes.values(), normalize=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what parameters are available and have been set by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All objects in the pipeline are stored in a dictonary. You can easily access them like any ordinary python dictionary. E.g. you want to transform only a document to a vector. This can be useful if you want to have the output of the preprocessing step for plotting or further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = pipeline1.get_params()['vectorizer']\n",
    "vectorizer.transform(['A new document']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a TF-IDF transformer that suppress the weight of common words and make special words more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('vectorizer', CountVectorizer()),\n",
    "         ('tfidf', TfidfTransformer()),\n",
    "         ('classifier', LinearSVC(random_state=1))]\n",
    "pipeline2 = train_and_test(steps, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('vectorizer', CountVectorizer(analyzer=stemmed)),\n",
    "         ('tfidf', TfidfTransformer()),\n",
    "         ('classifier', LinearSVC(random_state=1))]\n",
    "pipeline3 = train_and_test(steps, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('vectorizer', CountVectorizer(analyzer=stemmed, \n",
    "                                        stop_words='english')),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('classifier', LinearSVC(random_state=1))]\n",
    "pipeline4 = train_and_test(steps, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
    "         ('tfidf', TfidfTransformer()),\n",
    "         ('classifier', LinearSVC(random_state=1))]\n",
    "pipeline5 = train_and_test(steps, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
    "         ('tfidf', TfidfTransformer()),\n",
    "         ('classifier', LinearSVC(random_state=1))]\n",
    "pipeline6 = train_and_test(steps, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('vectorizer', CountVectorizer(preprocessor=mask_all, \n",
    "                                        analyzer=stemmed, \n",
    "                                        stop_words='english', \n",
    "                                        ngram_range=(1, 3))),\n",
    "         ('tfidf', TfidfTransformer()),\n",
    "         ('classifier', LinearSVC(random_state=1))]\n",
    "pipeline7 = train_and_test(steps, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One vs Rest Classifiers\n",
    "\n",
    "Let's train 8 classifiers: One vs Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "ym_train = enc.fit_transform(np.array(y_train.tolist()).reshape(-1,1))\n",
    "ym_test = enc.transform(np.array(y_test.tolist()).reshape(-1,1))\n",
    "\n",
    "steps = [('vectorizer', CountVectorizer(analyzer=stemmed, \n",
    "                                        stop_words='english', \n",
    "                                        preprocessor=mask_all)),\n",
    "         ('transformer', TfidfTransformer()),\n",
    "         ('classifier', OneVsRestClassifier(LinearSVC(random_state=1, multi_class='ovr')))]\n",
    "pipeline8 = train_and_test(steps, X_train, X_test, ym_train, ym_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "steps = [('vectorizer', CountVectorizer(ngram_range=(1,3), \n",
    "                                        analyzer=stemmed, \n",
    "                                        stop_words='english', \n",
    "                                        preprocessor=mask_all)),\n",
    "         ('transformer', TfidfTransformer()),\n",
    "         ('classifier', OneVsOneClassifier(LinearSVC(random_state=1, multi_class='ovr')))]\n",
    "pipeline9 = train_and_test(steps, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline9.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=dict_classes.values(), normalize=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "steps = [('vectorizer', CountVectorizer()),\n",
    "         ('transformer', TfidfTransformer()),\n",
    "         ('classifier', SVC(kernel='linear', random_state=1))]\n",
    "pipeline10 = Pipeline(steps)\n",
    "\n",
    "params = {\n",
    "    'vectorizer__tokenizer': [None, nltk.tokenize.word_tokenize],\n",
    "    'vectorizer__analyzer': ['word', stemmed],\n",
    "    'vectorizer__stop_words': [None, nltk.corpus.stopwords.words('english'), 'english'],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2), (1, 3)],\n",
    "    'vectorizer__preprocessor': [None, mask_all, mask_integers, mask_times, mask_emojis],\n",
    "    'classifier__C': np.logspace(-2, 2, 5),\n",
    "    'classifier__gamma': np.logspace(-5, 3, 9)\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipeline10, params, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_params_)\n",
    "y_pred = gs.predict(X_test)\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test))\n",
    "print('Score on the test set: {:.2f}'.format(gs.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try a gridsearch with another classifier. But it is hard to beat Linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "steps = [('vectorizer', CountVectorizer()),\n",
    "         ('transformer', TfidfTransformer()),\n",
    "         ('classifier', RandomForestClassifier())]\n",
    "pipeline11 = Pipeline(steps)\n",
    "\n",
    "params = {\n",
    "    'vectorizer__tokenizer': [None, nltk.tokenize.word_tokenize],\n",
    "    'vectorizer__analyzer': ['word', stemmed],\n",
    "    'vectorizer__stop_words': [None, nltk.corpus.stopwords.words('english'), 'english'],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2), (1, 3)],\n",
    "    'vectorizer__preprocessor': [None, mask_all, mask_integers, mask_times, mask_emojis]\n",
    "    'classifier__n_estimators': [128, 512, 2048],\n",
    "    'classifier__bootstrap': [True, False],\n",
    "    'classifier__criterion': ['gini', 'entropy'], \n",
    "    'classifier__max_depth': [2, 4, 8, None],\n",
    "    'classifier__min_samples_leaf': [1, 3, 10], \n",
    "    'classifier__min_samples_split': [2, 8, 16]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipeline11, params, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_params_)\n",
    "y_pred = gs.predict(X_test)\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test))\n",
    "print('Score on the test set: {:.2f}'.format(gs.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = gs\n",
    "\n",
    "tag_message(pipeline, 'My alarm clock was not set properly. I come to the office asap.')\n",
    "tag_message(pipeline, 'It is my scheduled day off, see you on Tuesday.')\n",
    "tag_message(pipeline, 'Not feeling well today, I stay home and work from here.')\n",
    "tag_message(pipeline, 'I work at home on Tuesday.')\n",
    "tag_message(pipeline, 'This morning I have a meeting at SPS.')\n",
    "tag_message(pipeline, 'I\\'m off, see you tomorrow.')\n",
    "tag_message(pipeline, 'get well soon!')\n",
    "tag_message(pipeline, 'I\\'m away for a long lunch between 12:00 and 15:30')\n",
    "tag_message(pipeline, 'I\\'ve an appointment at 12:00 at the physiotherapy.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}