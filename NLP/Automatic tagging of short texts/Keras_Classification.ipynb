{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message Classifier, Multi class with a Neural Network\n",
    "\n",
    "Gilbert Fran√ßois Duivesteijn (gilbert@deep-impact.ch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dt140704.gif\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import itertools\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from lib.utils import plot_confusion_matrix\n",
    "\n",
    "np.set_printoptions(precision=3, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cls_messages = pd.read_pickle('data/messages-cls-ms.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classes = {\n",
    "    0: 'late/early',\n",
    "    1: 'holidays',\n",
    "    2: 'home office',\n",
    "    3: 'med app',\n",
    "    4: 'ill',\n",
    "    5: 'business',\n",
    "    6: 'in office',\n",
    "    7: 'miscellaneous'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df_cls_messages.dropna()\n",
    "samples['class'] = samples.loc[:, 'class'].astype(np.uint8).values\n",
    "\n",
    "samples['text'] = samples['text'].str.lower()\n",
    "samples['text'] = samples['text'].str.replace('ho','home office')\n",
    "samples['text'] = samples['text'].str.replace('homeoffice','home office')\n",
    "\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = samples['text']\n",
    "y = samples['class']\n",
    "\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 3000\n",
    "maxlen = 50\n",
    "embedding_size = 100\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 20\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ', char_level=False)\n",
    "tokenizer.fit_on_texts(X)\n",
    "Xt = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "print(X.iloc[2])\n",
    "print(Xt[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index)\n",
    "print('Number of words in the corpus: {}'.format(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_list = np.array([len(x) for x in Xt])\n",
    "mean_len = np.mean(len_list)\n",
    "std_len = np.std(len_list)\n",
    "print(mean_len, std_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Label Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc = to_categorical(y-1)\n",
    "yc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence padding\n",
    "\n",
    "Make all input vectors the same size by truncating or adding zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xts = sequence.pad_sequences(Xt, maxlen=maxlen, padding='post')\n",
    "print(X[0], '\\n')\n",
    "print(Xt[0], '\\n')\n",
    "print(Xts[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xts, yc, test_size=0.10, random_state=1)\n",
    "\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('X_test shape : {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(X_train, X_test, y_train, y_test, verbose=1):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "    model.add(Dense(8))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    callback = EarlyStopping(monitor='val_acc', patience=2)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    print('Train...')\n",
    "    H = model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(X_test, y_test), callbacks=[callback], verbose=verbose)\n",
    "    score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    print('Test accuracy:', acc)\n",
    "    return model, H, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, H, acc = build_and_train_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].plot(H.history['acc'], label='acc')\n",
    "axs[0].plot(H.history['val_acc'], label='val_acc')\n",
    "axs[1].plot(H.history['loss'], label='loss')\n",
    "axs[1].plot(H.history['val_loss'], label='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=np.argmax(y_pred, axis=1), y_true=np.argmax(y_test, axis=1)))\n",
    "cm = confusion_matrix(y_pred=np.argmax(y_pred, axis=1), y_true=np.argmax(y_test, axis=1))\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=dict_classes.values(), normalize=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an ensemble\n",
    "\n",
    "Since we do early stopping, to minimise overfitting, the model is not fully trained yet. The effect of random initialisation is noticable. Hence, when training e.g. 10 times from scratch, you end up with 10 models with different performance. An idea to smooth this effect is to train multiple times and use all models for the prediction. Add all resulting output vectors and take the $argmax$. This will be your final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 10\n",
    "\n",
    "model_list = []\n",
    "H_list = []\n",
    "acc_list = []\n",
    "for i in range(n_models):\n",
    "    print('Training model {}'.format(i+1))\n",
    "    _model, _H, _acc = build_and_train_model(X_train, X_test, y_train, y_test, verbose=0)\n",
    "    model_list.append(_model)\n",
    "    H_list.append(_H)\n",
    "    acc_list.append(_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean test accuracy: {:.3f}'.format(np.mean(np.array(acc_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "for H in H_list:\n",
    "    axs[0].plot(H.history['acc'], label='acc')\n",
    "    axs[0].plot(H.history['val_acc'], label='val_acc')\n",
    "    axs[1].plot(H.history['loss'], label='loss')\n",
    "    axs[1].plot(H.history['val_loss'], label='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "for index in range(len(X_test)):\n",
    "    y_pred = np.zeros(8)\n",
    "    for model in model_list:\n",
    "        y_pred += model.predict(X_test[index][np.newaxis])[0]\n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "y_pred = np.array(y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=np.argmax(y_test, axis=1), y_pred=np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=np.argmax(y_pred, axis=1), y_true=np.argmax(y_test, axis=1)))\n",
    "print('Accuracy: {:.3f}'.format(accuracy_score(y_true=np.argmax(y_test, axis=1), y_pred=np.argmax(y_pred, axis=1))))\n",
    "cm = confusion_matrix(y_pred=np.argmax(y_pred, axis=1), y_true=np.argmax(y_test, axis=1))\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=dict_classes.values(), normalize=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_message(message):\n",
    "    X_new = tokenizer.texts_to_sequences([message])\n",
    "    x1_new = sequence.pad_sequences(X_new, maxlen=maxlen, padding='post')\n",
    "    y_pred_list = []\n",
    "    for model in model_list:\n",
    "        y_pred_list.append(model.predict(x1_new))\n",
    "    y_pred = np.argmax(np.sum(np.array(y_pred_list), axis=0) / 10.0)\n",
    "    print('{:>20} | {}'.format(dict_classes[y_pred], message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_message('My alarm clock was not set properly. I come to the office asap.')\n",
    "tag_message('It is my scheduled day off.')\n",
    "tag_message('See you on Wednesday.')\n",
    "tag_message('Not feeling well today, I had to vomit.')\n",
    "tag_message('I work at home on Tuesday.')\n",
    "tag_message('This morning I have a meeting at ACME.')\n",
    "tag_message('The roads are super slippery, will miss the daily')\n",
    "tag_message('get well soon!')\n",
    "tag_message('I\\'m away for a long lunch between 12:00 and 15:30')\n",
    "tag_message('I have an appointment at the physio.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning with pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove/glove.6B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index['home'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Input(shape=(maxlen,), dtype='int32'))\n",
    "model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "callback = EarlyStopping(monitor='val_acc', patience=1)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "H = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test), callbacks=[callback])\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=np.argmax(y_pred, axis=1), y_true=np.argmax(y_test, axis=1)))\n",
    "cm = confusion_matrix(y_pred=np.argmax(y_pred, axis=1), y_true=np.argmax(y_test, axis=1))\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=dict_classes.values(), normalize=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}