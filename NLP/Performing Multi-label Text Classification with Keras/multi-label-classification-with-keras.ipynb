{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi-label-classification-with-keras.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "_uuid": "3069af84cb76189b9bf10874ba99d6d1e237bbe5",
        "id": "0fp_i_DpzeEt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Multi-label text classification with keras\n",
        "## by Rocco Schulz\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "B4vzMz8BzrQE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://blog.mimacom.com/text-classification/"
      ]
    },
    {
      "metadata": {
        "id": "sOswLb8Gzsr8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/multi-label-classification-with-keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/multi-label-classification-with-keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ot025wGezeEw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "\n",
        "import os\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "20X58OAC315O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "dc6cbbac-52ff-44dc-f4e1-616dbc247c24"
      },
      "cell_type": "code",
      "source": [
        "# Download data files\n",
        "!wget https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/data/Questions.csv?raw=true\n",
        "!wget https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/data/Tags.csv?raw=true\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-04 17:22:39--  https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/data/Questions.csv?raw=true\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/learning-stack/Colab-ML-Playbook/raw/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/data/Questions.csv [following]\n",
            "--2019-01-04 17:22:39--  https://github.com/learning-stack/Colab-ML-Playbook/raw/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/data/Questions.csv\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/learning-stack/Colab-ML-Playbook/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/data/Questions.csv [following]\n",
            "--2019-01-04 17:22:39--  https://raw.githubusercontent.com/learning-stack/Colab-ML-Playbook/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/data/Questions.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 104276279 (99M) [text/plain]\n",
            "Saving to: ‘Questions.csv?raw=true’\n",
            "\n",
            "Questions.csv?raw=t 100%[===================>]  99.45M  52.1MB/s    in 1.9s    \n",
            "\n",
            "2019-01-04 17:22:42 (52.1 MB/s) - ‘Questions.csv?raw=true’ saved [104276279/104276279]\n",
            "\n",
            "--2019-01-04 17:22:42--  https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/data/Tags.csv?raw=true\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/learning-stack/Colab-ML-Playbook/raw/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/data/Tags.csv [following]\n",
            "--2019-01-04 17:22:42--  https://github.com/learning-stack/Colab-ML-Playbook/raw/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/data/Tags.csv\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/learning-stack/Colab-ML-Playbook/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/data/Tags.csv [following]\n",
            "--2019-01-04 17:22:43--  https://raw.githubusercontent.com/learning-stack/Colab-ML-Playbook/master/NLP/Performing%20Multi-label%20Text%20Classification%20with%20Keras/data/Tags.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4653766 (4.4M) [text/plain]\n",
            "Saving to: ‘Tags.csv?raw=true’\n",
            "\n",
            "Tags.csv?raw=true   100%[===================>]   4.44M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-01-04 17:22:43 (42.5 MB/s) - ‘Tags.csv?raw=true’ saved [4653766/4653766]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "vgGHBOPbzeE0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "dcc2c996-05d8-48e3-b715-90aa10e631d4"
      },
      "cell_type": "code",
      "source": [
        "df_questions = pd.read_csv('Questions.csv?raw=true', encoding='iso-8859-1')\n",
        "df_tags = pd.read_csv('Tags.csv?raw=true', encoding='iso-8859-1')\n",
        "df_questions.head(n=2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>OwnerUserId</th>\n",
              "      <th>CreationDate</th>\n",
              "      <th>Score</th>\n",
              "      <th>Title</th>\n",
              "      <th>Body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2010-07-19T19:14:44Z</td>\n",
              "      <td>272</td>\n",
              "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
              "      <td>&lt;p&gt;Last year, I read a blog post from &lt;a href=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21</td>\n",
              "      <td>59.0</td>\n",
              "      <td>2010-07-19T19:24:36Z</td>\n",
              "      <td>4</td>\n",
              "      <td>Forecasting demographic census</td>\n",
              "      <td>&lt;p&gt;What are some of the ways to forecast demog...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  OwnerUserId          CreationDate  Score  \\\n",
              "0   6          5.0  2010-07-19T19:14:44Z    272   \n",
              "1  21         59.0  2010-07-19T19:24:36Z      4   \n",
              "\n",
              "                                               Title  \\\n",
              "0  The Two Cultures: statistics vs. machine learn...   \n",
              "1                     Forecasting demographic census   \n",
              "\n",
              "                                                Body  \n",
              "0  <p>Last year, I read a blog post from <a href=...  \n",
              "1  <p>What are some of the ways to forecast demog...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3cb05d6b416914c26d66531e8e521f5e412c11af",
        "id": "7xON3PQqzeE3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f517a991-6417-4709-d910-0ef5bd4cb00c"
      },
      "cell_type": "code",
      "source": [
        "grouped_tags = df_tags.groupby(\"Tag\", sort='count').size().reset_index(name='count')\n",
        "grouped_tags.Tag.describe()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count                1315\n",
              "unique               1315\n",
              "top       crostons-method\n",
              "freq                    1\n",
              "Name: Tag, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "93f06b61db6e2b42fd6c1537841f919892211975",
        "id": "krZ-L2etzeE6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reducing the problem to the most common tags in the dataset\n",
        "For rare tags there were simply not enough samples available to get reliable results, thus only the top 100 tags were kept. But even with only the 100 most frequently used tags there is still an imbalance as some tags are used more often than others.\n",
        "\n",
        " ![first vs last 5 tags](https://blog.mimacom.com/en/media/ef1a71298726a6f3babd46e9e2988fa1/first_vs_last_5_tags.png)\n",
        "\n",
        "To address this imbalance we calculated class weights to be used as parameters for the loss function of our model. By multiplying the class weights with the categorical losses we can counter the imbalance, so that making false classifications for the tag algorithms is equally expensive as for the tag r.\n",
        "The calculated class weights are plotted against the counts of the tags below.\n",
        "\n",
        " ![class weights plotted vs class observations](https://blog.mimacom.com/en/media/ba1508ba102ac1f7ddb254e9ba06d5a3/class-weights.png)\n",
        "\n",
        "There are alternative ways to address class imbalances. We could also have used resampling to duplicate samples in under-represented classes or reduce the number of samples in over-represented classes or trained several models with balanced subsets of the data and model averaging. (cf. Longadge2013) For resampling there is a scikit-learn compatible library imbalanced-learn which also illustrates the class imbalance problem and supported resampling strategies in its documentation."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7af43d023d87159a669f6678dc79f4886fb677e9",
        "id": "xi5FtHcrzeE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        },
        "outputId": "29667cc9-d51b-4aea-c120-ff4aeea29c4b"
      },
      "cell_type": "code",
      "source": [
        "num_classes = 100\n",
        "grouped_tags = df_tags.groupby(\"Tag\").size().reset_index(name='count')\n",
        "most_common_tags = grouped_tags.nlargest(num_classes, columns=\"count\")\n",
        "df_tags.Tag = df_tags.Tag.apply(lambda tag : tag if tag in most_common_tags.Tag.values else None)\n",
        "df_tags = df_tags.dropna()\n",
        "df_tags"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>bayesian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>distributions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "      <td>distributions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4</td>\n",
              "      <td>statistical-significance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6</td>\n",
              "      <td>machine-learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>7</td>\n",
              "      <td>dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>10</td>\n",
              "      <td>ordinal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>17</td>\n",
              "      <td>anova</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>17</td>\n",
              "      <td>chi-squared</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>17</td>\n",
              "      <td>generalized-linear-model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>21</td>\n",
              "      <td>forecasting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>22</td>\n",
              "      <td>bayesian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>23</td>\n",
              "      <td>distributions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>23</td>\n",
              "      <td>pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>25</td>\n",
              "      <td>modeling</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>25</td>\n",
              "      <td>time-series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>26</td>\n",
              "      <td>standard-deviation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>30</td>\n",
              "      <td>algorithms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>30</td>\n",
              "      <td>hypothesis-testing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>30</td>\n",
              "      <td>random-variable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>31</td>\n",
              "      <td>hypothesis-testing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>31</td>\n",
              "      <td>t-test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>31</td>\n",
              "      <td>p-value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>31</td>\n",
              "      <td>interpretation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>33</td>\n",
              "      <td>r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>35</td>\n",
              "      <td>distributions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>35</td>\n",
              "      <td>modeling</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>35</td>\n",
              "      <td>poisson</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>36</td>\n",
              "      <td>correlation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>39</td>\n",
              "      <td>modeling</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244182</th>\n",
              "      <td>241455</td>\n",
              "      <td>distributions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244184</th>\n",
              "      <td>241458</td>\n",
              "      <td>regression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244185</th>\n",
              "      <td>241458</td>\n",
              "      <td>multiple-regression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244186</th>\n",
              "      <td>241458</td>\n",
              "      <td>repeated-measures</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244187</th>\n",
              "      <td>241459</td>\n",
              "      <td>spss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244188</th>\n",
              "      <td>241459</td>\n",
              "      <td>repeated-measures</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244189</th>\n",
              "      <td>241461</td>\n",
              "      <td>probability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244190</th>\n",
              "      <td>241461</td>\n",
              "      <td>sampling</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244193</th>\n",
              "      <td>241464</td>\n",
              "      <td>regression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244194</th>\n",
              "      <td>241464</td>\n",
              "      <td>regression-coefficients</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244195</th>\n",
              "      <td>241464</td>\n",
              "      <td>residuals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244197</th>\n",
              "      <td>241465</td>\n",
              "      <td>statistical-significance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244198</th>\n",
              "      <td>241465</td>\n",
              "      <td>modeling</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244202</th>\n",
              "      <td>241466</td>\n",
              "      <td>regression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244203</th>\n",
              "      <td>241466</td>\n",
              "      <td>hypothesis-testing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244204</th>\n",
              "      <td>241466</td>\n",
              "      <td>correlation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244206</th>\n",
              "      <td>241468</td>\n",
              "      <td>distributions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244207</th>\n",
              "      <td>241468</td>\n",
              "      <td>statistical-significance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244208</th>\n",
              "      <td>241468</td>\n",
              "      <td>poisson</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244210</th>\n",
              "      <td>241470</td>\n",
              "      <td>probability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244211</th>\n",
              "      <td>241470</td>\n",
              "      <td>mathematical-statistics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244214</th>\n",
              "      <td>241472</td>\n",
              "      <td>confidence-interval</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244215</th>\n",
              "      <td>241473</td>\n",
              "      <td>r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244216</th>\n",
              "      <td>241473</td>\n",
              "      <td>optimization</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244220</th>\n",
              "      <td>241475</td>\n",
              "      <td>categorical-data</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244222</th>\n",
              "      <td>241479</td>\n",
              "      <td>r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244223</th>\n",
              "      <td>241479</td>\n",
              "      <td>machine-learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244224</th>\n",
              "      <td>241480</td>\n",
              "      <td>mathematical-statistics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244225</th>\n",
              "      <td>241481</td>\n",
              "      <td>normal-distribution</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244226</th>\n",
              "      <td>241481</td>\n",
              "      <td>estimation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>152913 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Id                       Tag\n",
              "0            1                  bayesian\n",
              "3            2             distributions\n",
              "7            4             distributions\n",
              "8            4  statistical-significance\n",
              "9            6          machine-learning\n",
              "10           7                   dataset\n",
              "16          10                   ordinal\n",
              "21          17                     anova\n",
              "22          17               chi-squared\n",
              "23          17  generalized-linear-model\n",
              "24          21               forecasting\n",
              "27          22                  bayesian\n",
              "29          23             distributions\n",
              "30          23                       pdf\n",
              "32          25                  modeling\n",
              "33          25               time-series\n",
              "36          26        standard-deviation\n",
              "38          30                algorithms\n",
              "39          30        hypothesis-testing\n",
              "40          30           random-variable\n",
              "42          31        hypothesis-testing\n",
              "43          31                    t-test\n",
              "44          31                   p-value\n",
              "45          31            interpretation\n",
              "47          33                         r\n",
              "49          35             distributions\n",
              "50          35                  modeling\n",
              "51          35                   poisson\n",
              "53          36               correlation\n",
              "55          39                  modeling\n",
              "...        ...                       ...\n",
              "244182  241455             distributions\n",
              "244184  241458                regression\n",
              "244185  241458       multiple-regression\n",
              "244186  241458         repeated-measures\n",
              "244187  241459                      spss\n",
              "244188  241459         repeated-measures\n",
              "244189  241461               probability\n",
              "244190  241461                  sampling\n",
              "244193  241464                regression\n",
              "244194  241464   regression-coefficients\n",
              "244195  241464                 residuals\n",
              "244197  241465  statistical-significance\n",
              "244198  241465                  modeling\n",
              "244202  241466                regression\n",
              "244203  241466        hypothesis-testing\n",
              "244204  241466               correlation\n",
              "244206  241468             distributions\n",
              "244207  241468  statistical-significance\n",
              "244208  241468                   poisson\n",
              "244210  241470               probability\n",
              "244211  241470   mathematical-statistics\n",
              "244214  241472       confidence-interval\n",
              "244215  241473                         r\n",
              "244216  241473              optimization\n",
              "244220  241475          categorical-data\n",
              "244222  241479                         r\n",
              "244223  241479          machine-learning\n",
              "244224  241480   mathematical-statistics\n",
              "244225  241481       normal-distribution\n",
              "244226  241481                estimation\n",
              "\n",
              "[152913 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "340f6c140f1e69676e5c63c5af2cc3c9ff31be7d",
        "id": "UUpZS-7-zeE_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the contents of the dataframe\n",
        "\n",
        "The question body contains html tags that we don't want to feed into our model. We will thus strip all tags and combine title and question body into a single field for simplicity."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9bed5c95c0a39c535bfb36e721272c40a42077f5",
        "id": "UABr0t1nzeFA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re \n",
        "\n",
        "def strip_html_tags(body):\n",
        "    regex = re.compile('<.*?>')\n",
        "    return re.sub(regex, '', body)\n",
        "\n",
        "df_questions['Body'] = df_questions['Body'].apply(strip_html_tags)\n",
        "df_questions['Text'] = df_questions['Title'] + ' ' + df_questions['Body']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "361bcf1624ad1c6c15aa6287772cd1acb6182368",
        "id": "2kJXRwtPzeFD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# denormalize tables\n",
        "\n",
        "def tags_for_question(question_id):\n",
        "    return df_tags[df_tags['Id'] == question_id].Tag.values\n",
        "\n",
        "def add_tags_column(row):\n",
        "    row['Tags'] = tags_for_question(row['Id'])\n",
        "    return row\n",
        "\n",
        "df_questions = df_questions.apply(add_tags_column, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e3788b41a2b7eae538146f9f59dd2c08ede44889",
        "id": "P7XHIPMozeFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "e6638c39-449f-4d41-cac5-8a668f20d785"
      },
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', 400)\n",
        "df_questions[['Id', 'Text', 'Tags']].head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Text</th>\n",
              "      <th>Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>The Two Cultures: statistics vs. machine learning? Last year, I read a blog post from Brendan O'Connor entitled \"Statistics vs. Machine Learning, fight!\" that discussed some of the differences between the two fields.  Andrew Gelman responded favorably to this:\\n\\nSimon Blomberg: \\n\\n\\n  From R's fortunes\\n  package: To paraphrase provocatively,\\n  'machine learning is statistics minus\\n  any c...</td>\n",
              "      <td>[machine-learning]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21</td>\n",
              "      <td>Forecasting demographic census What are some of the ways to forecast demographic census with some validation and calibration techniques?\\n\\nSome of the concerns:\\n\\n\\nCensus blocks vary in sizes as rural\\nareas are a lot larger than condensed\\nurban areas. Is there a need to account for the area size difference?\\nif let's say I have census data\\ndating back to 4 - 5 census periods,\\nhow far ca...</td>\n",
              "      <td>[forecasting]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>Bayesian and frequentist reasoning in plain English How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?\\n</td>\n",
              "      <td>[bayesian]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31</td>\n",
              "      <td>What is the meaning of p values and t values in statistical tests? After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests.  It seems that students easily learn how to perform the calculations required by a given test but get hung up on interpreting the resul...</td>\n",
              "      <td>[hypothesis-testing, t-test, p-value, interpretation]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>36</td>\n",
              "      <td>Examples for teaching: Correlation does not mean causation There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:\\n\\n\\nnumber of storks and birth rate in Denmark;\\nnumber of priests in America and alcoholism;\\nin the start of the 20th century it was noted that there was a strong correlation between 'N...</td>\n",
              "      <td>[correlation]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  \\\n",
              "0   6   \n",
              "1  21   \n",
              "2  22   \n",
              "3  31   \n",
              "4  36   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                              Text  \\\n",
              "0  The Two Cultures: statistics vs. machine learning? Last year, I read a blog post from Brendan O'Connor entitled \"Statistics vs. Machine Learning, fight!\" that discussed some of the differences between the two fields.  Andrew Gelman responded favorably to this:\\n\\nSimon Blomberg: \\n\\n\\n  From R's fortunes\\n  package: To paraphrase provocatively,\\n  'machine learning is statistics minus\\n  any c...   \n",
              "1  Forecasting demographic census What are some of the ways to forecast demographic census with some validation and calibration techniques?\\n\\nSome of the concerns:\\n\\n\\nCensus blocks vary in sizes as rural\\nareas are a lot larger than condensed\\nurban areas. Is there a need to account for the area size difference?\\nif let's say I have census data\\ndating back to 4 - 5 census periods,\\nhow far ca...   \n",
              "2                                                                                                                                                                                                                                          Bayesian and frequentist reasoning in plain English How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?\\n   \n",
              "3  What is the meaning of p values and t values in statistical tests? After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests.  It seems that students easily learn how to perform the calculations required by a given test but get hung up on interpreting the resul...   \n",
              "4  Examples for teaching: Correlation does not mean causation There is an old saying: \"Correlation does not mean causation\". When I teach, I tend to use the following standard examples to illustrate this point:\\n\\n\\nnumber of storks and birth rate in Denmark;\\nnumber of priests in America and alcoholism;\\nin the start of the 20th century it was noted that there was a strong correlation between 'N...   \n",
              "\n",
              "                                                    Tags  \n",
              "0                                     [machine-learning]  \n",
              "1                                          [forecasting]  \n",
              "2                                             [bayesian]  \n",
              "3  [hypothesis-testing, t-test, p-value, interpretation]  \n",
              "4                                          [correlation]  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "80afc34addf7ea5b6e4d08c3af3568d57e21a626",
        "id": "3b0wE_m_zeFL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the text\n",
        "The text has to be vectorized so that we can feed it into our model. Keras comes with [several text preprocessing classes](https://keras.io/preprocessing/text/) that we can use for that.\n",
        "\n",
        "The labels need encoded as well, so that the 100 labels will be represented as 100 binary values in an array. This can be done with the [MultiLabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html) from the sklearn library."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8bfc23b1e715e60425e861f855d9848e97942069",
        "id": "3crroU22zeFN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "multilabel_binarizer = MultiLabelBinarizer()\n",
        "multilabel_binarizer.fit(df_questions.Tags)\n",
        "labels = multilabel_binarizer.classes_\n",
        "\n",
        "maxlen = 180\n",
        "max_words = 5000\n",
        "tokenizer = Tokenizer(num_words=max_words, lower=True)\n",
        "tokenizer.fit_on_texts(df_questions.Text)\n",
        "\n",
        "def get_features(text_series):\n",
        "    \"\"\"\n",
        "    transforms text data to feature_vectors that can be used in the ml model.\n",
        "    tokenizer must be available.\n",
        "    \"\"\"\n",
        "    sequences = tokenizer.texts_to_sequences(text_series)\n",
        "    return pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "\n",
        "def prediction_to_label(prediction):\n",
        "    tag_prob = [(labels[i], prob) for i, prob in enumerate(prediction.tolist())]\n",
        "    return dict(sorted(tag_prob, key=lambda kv: kv[1], reverse=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gokrAQFN6dta",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the snippet above only the most frequent 5000 words are used to build a dictionary. We limit the sequence length to 180 words.\n",
        "\n",
        "The labels need to be encoded as well, so that the 100 labels will be represented as 100 binary elements in an array. This was done with the MultiLabelBinarizer from the sklearn library.\n",
        "\n",
        "Finally we can split our data into training and test set to conclude the data preparation:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ee229fc276c7ef58ee21e06e5d9694bea7f81c8",
        "id": "MU2Xq6LkzeFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "563aa16e-cd7c-4bc4-f59f-d805275f9754"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = get_features(df_questions.Text)\n",
        "y = multilabel_binarizer.transform(df_questions.Tags)\n",
        "print(x.shape)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=9000)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(85085, 180)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "0148c095b2f97a53511097f560a07124c2136efb",
        "id": "gEdfGbuRzeFU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imbalanced Classes\n",
        "Some tags occur more often than others, thus the classes are not well balanced. The imbalanced class problem can be addressed by applying class weights, thus  weighting less frequent tags higher than very frequent tags."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5f0d7d29d627b5f8cd7f18486c3e9cfe93955e9",
        "id": "uYN7owXYzeFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7a453349-a48c-4377-d62b-9e07739a911f"
      },
      "cell_type": "code",
      "source": [
        "most_common_tags['class_weight'] = len(df_tags) / most_common_tags['count']\n",
        "class_weight = {}\n",
        "for index, label in enumerate(labels):\n",
        "    class_weight[index] = most_common_tags[most_common_tags['Tag'] == label]['class_weight'].values[0]\n",
        "    \n",
        "most_common_tags.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tag</th>\n",
              "      <th>count</th>\n",
              "      <th>class_weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>r</td>\n",
              "      <td>13236</td>\n",
              "      <td>11.552811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>regression</td>\n",
              "      <td>10959</td>\n",
              "      <td>13.953189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>machine-learning</td>\n",
              "      <td>6089</td>\n",
              "      <td>25.112991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>time-series</td>\n",
              "      <td>5559</td>\n",
              "      <td>27.507285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>probability</td>\n",
              "      <td>4217</td>\n",
              "      <td>36.261086</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Tag  count  class_weight\n",
              "74                 r  13236     11.552811\n",
              "79        regression  10959     13.953189\n",
              "41  machine-learning   6089     25.112991\n",
              "98       time-series   5559     27.507285\n",
              "71       probability   4217     36.261086"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "9be5f3113d27c68fc9456cb18ba9d38d94bb0ebf",
        "id": "DAL-muKtzeFX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Creation\n",
        "\n",
        "There are various approaches to classify text and in practice you should try several of them to see which one works best for the task at hand. For brevity we will focus on Keras in this article, but we encourage you to try LightGBM, Support Vector Machines or Logistic Regression with n-grams or tf-idf input features. The latter shallow classifiers can be created as binary classifiers - one for each category. By running all of them one can determine probabilities for each category. Sklearn comes with the OneVsRestClassifier which supports this strategy. This is briefly demonstrated in our notebook [multi-label classification](https://www.kaggle.com/roccoli/multi-label-classification-with-sklearn) with sklearn on Kaggle which you may use as a starting point for further experimentation.\n",
        "\n",
        "## Word Embeddings\n",
        "In the previous steps we tokenized our text and vectorized the resulting tokens using one-hot encoding. The resulting vectors are sparse, binary representations which mainly contain zeros and are high-dimensional (depending on the number of unique words in the vocabulary).\n",
        "\n",
        "Word embeddings on the other hand are low dimensional as they represent tokens as dense floating point vectors and thus pack more information into fewer dimensions. Words with similar meanings are associated with similar representations. Word embeddings can be obtained in two ways:\n",
        "\n",
        "1.   Learn word embeddings together with the weights of the neural network\n",
        "2.   Load pretrained word embeddings which were precomputed as part of a different machine learning task.\n",
        "\n",
        "We decided to learn new word embeddings as the dataset contains vocabulary which is specific to the domain of statistics and we didn't expect to benefit from pretrained embeddings which use a broader vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "NcrtGQ5u8KwJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Simple Baseline\n",
        "We started with a simple model which only consists of an embedding layer, a dropout layer to reduce the size and prevent overfitting, a max pooling layer and one dense layer with a sigmoid activation to produce probabilities for each of the 100 classes that we want to predict."
      ]
    },
    {
      "metadata": {
        "id": "BW_lIIEs8Pjt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "840440c1-e20a-4db7-ee43-03e78e7ba019"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, GlobalMaxPool1D, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 20, input_length=maxlen))\n",
        "model.add(Dropout(0.15))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(num_classes, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(0.015), loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(),\n",
        "    EarlyStopping(patience=4),\n",
        "    ModelCheckpoint(filepath='model-simple.h5', save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    class_weight=class_weight,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=callbacks)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 61261 samples, validate on 6807 samples\n",
            "Epoch 1/20\n",
            "61261/61261 [==============================] - 28s 452us/step - loss: 10.0273 - categorical_accuracy: 0.0745 - val_loss: 9.2570 - val_categorical_accuracy: 0.1018\n",
            "Epoch 2/20\n",
            "61261/61261 [==============================] - 27s 448us/step - loss: 8.6442 - categorical_accuracy: 0.1289 - val_loss: 8.4250 - val_categorical_accuracy: 0.1487\n",
            "Epoch 3/20\n",
            "61261/61261 [==============================] - 27s 447us/step - loss: 8.1523 - categorical_accuracy: 0.1551 - val_loss: 8.1766 - val_categorical_accuracy: 0.1447\n",
            "Epoch 4/20\n",
            "61261/61261 [==============================] - 27s 448us/step - loss: 7.8870 - categorical_accuracy: 0.1744 - val_loss: 7.9638 - val_categorical_accuracy: 0.1833\n",
            "Epoch 5/20\n",
            "61261/61261 [==============================] - 27s 446us/step - loss: 7.7290 - categorical_accuracy: 0.1822 - val_loss: 7.7830 - val_categorical_accuracy: 0.1664\n",
            "Epoch 6/20\n",
            "61261/61261 [==============================] - 27s 447us/step - loss: 7.5562 - categorical_accuracy: 0.1884 - val_loss: 7.6203 - val_categorical_accuracy: 0.1742\n",
            "Epoch 7/20\n",
            "61261/61261 [==============================] - 27s 446us/step - loss: 7.4694 - categorical_accuracy: 0.1959 - val_loss: 7.6327 - val_categorical_accuracy: 0.1728\n",
            "Epoch 8/20\n",
            "61261/61261 [==============================] - 27s 446us/step - loss: 7.4082 - categorical_accuracy: 0.1980 - val_loss: 7.5043 - val_categorical_accuracy: 0.1949\n",
            "Epoch 9/20\n",
            "61261/61261 [==============================] - 28s 454us/step - loss: 7.3629 - categorical_accuracy: 0.2067 - val_loss: 7.4185 - val_categorical_accuracy: 0.2007\n",
            "Epoch 10/20\n",
            "61261/61261 [==============================] - 27s 447us/step - loss: 7.3129 - categorical_accuracy: 0.2071 - val_loss: 7.3853 - val_categorical_accuracy: 0.2033\n",
            "Epoch 11/20\n",
            "61261/61261 [==============================] - 28s 458us/step - loss: 7.2571 - categorical_accuracy: 0.2148 - val_loss: 7.3532 - val_categorical_accuracy: 0.2026\n",
            "Epoch 12/20\n",
            "61261/61261 [==============================] - 28s 449us/step - loss: 7.2392 - categorical_accuracy: 0.2150 - val_loss: 7.3869 - val_categorical_accuracy: 0.2139\n",
            "Epoch 13/20\n",
            "61261/61261 [==============================] - 27s 447us/step - loss: 7.2330 - categorical_accuracy: 0.2151 - val_loss: 7.4133 - val_categorical_accuracy: 0.2052\n",
            "Epoch 14/20\n",
            "61261/61261 [==============================] - 29s 477us/step - loss: 7.2265 - categorical_accuracy: 0.2151 - val_loss: 7.3400 - val_categorical_accuracy: 0.2067\n",
            "Epoch 15/20\n",
            "61261/61261 [==============================] - 28s 456us/step - loss: 7.2188 - categorical_accuracy: 0.2155 - val_loss: 7.3454 - val_categorical_accuracy: 0.1958\n",
            "Epoch 16/20\n",
            "61261/61261 [==============================] - 28s 460us/step - loss: 7.2312 - categorical_accuracy: 0.2148 - val_loss: 7.3759 - val_categorical_accuracy: 0.2054\n",
            "Epoch 17/20\n",
            "61261/61261 [==============================] - 28s 461us/step - loss: 7.2268 - categorical_accuracy: 0.2158 - val_loss: 7.3355 - val_categorical_accuracy: 0.2019\n",
            "Epoch 18/20\n",
            "61261/61261 [==============================] - 28s 458us/step - loss: 7.2327 - categorical_accuracy: 0.2174 - val_loss: 7.3314 - val_categorical_accuracy: 0.2019\n",
            "Epoch 19/20\n",
            "61261/61261 [==============================] - 28s 460us/step - loss: 7.2358 - categorical_accuracy: 0.2157 - val_loss: 7.3209 - val_categorical_accuracy: 0.2124\n",
            "Epoch 20/20\n",
            "61261/61261 [==============================] - 29s 465us/step - loss: 7.2359 - categorical_accuracy: 0.2156 - val_loss: 7.3268 - val_categorical_accuracy: 0.2208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ssHEvI8CJPDx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4e236934-6314-48d3-e934-fa5a65e3606a"
      },
      "cell_type": "code",
      "source": [
        "# With this simple model the categorical accuracy was 22 % on the held out test dataset. This is better than guessing but not really satisfactory.\n",
        "simple_model = keras.models.load_model('model-simple.h5')\n",
        "metrics = simple_model.evaluate(x_test, y_test)\n",
        "print(\"{}: {}\".format(simple_model.metrics_names[0], metrics[0]))\n",
        "print(\"{}: {}\".format(simple_model.metrics_names[1], metrics[1]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17017/17017 [==============================] - 1s 47us/step\n",
            "loss: 0.06268962885289744\n",
            "categorical_accuracy: 0.21660692249102675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0_bvi6NH7vyf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1D Convolutional Neural Network"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e63351f66ae4bbb63aefd00279d8351e8f7fa8c",
        "id": "DYUnGTTtzeFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "d4d955ac-5199-483f-e8d5-a6b1d3cc420b"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "filter_length = 300\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 20, input_length=maxlen))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv1D(filter_length, 3, padding='valid', activation='relu', strides=1))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(), \n",
        "    EarlyStopping(patience=4), \n",
        "    ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    class_weight=class_weight,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 180, 20)           100000    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 180, 20)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 178, 300)          18300     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 100)               0         \n",
            "=================================================================\n",
            "Total params: 148,400\n",
            "Trainable params: 148,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 61261 samples, validate on 6807 samples\n",
            "Epoch 1/20\n",
            "61261/61261 [==============================] - 260s 4ms/step - loss: 10.6580 - categorical_accuracy: 0.0565 - val_loss: 9.1334 - val_categorical_accuracy: 0.0834\n",
            "Epoch 2/20\n",
            "61261/61261 [==============================] - 260s 4ms/step - loss: 8.0426 - categorical_accuracy: 0.1602 - val_loss: 7.3983 - val_categorical_accuracy: 0.2245\n",
            "Epoch 3/20\n",
            "61261/61261 [==============================] - 259s 4ms/step - loss: 6.6703 - categorical_accuracy: 0.2718 - val_loss: 6.5840 - val_categorical_accuracy: 0.2993\n",
            "Epoch 4/20\n",
            "61261/61261 [==============================] - 258s 4ms/step - loss: 6.0526 - categorical_accuracy: 0.3117 - val_loss: 6.1794 - val_categorical_accuracy: 0.3229\n",
            "Epoch 5/20\n",
            "61261/61261 [==============================] - 257s 4ms/step - loss: 5.7088 - categorical_accuracy: 0.3266 - val_loss: 6.1258 - val_categorical_accuracy: 0.3191\n",
            "Epoch 6/20\n",
            "61261/61261 [==============================] - 257s 4ms/step - loss: 5.4726 - categorical_accuracy: 0.3358 - val_loss: 6.0956 - val_categorical_accuracy: 0.3214\n",
            "Epoch 7/20\n",
            "61261/61261 [==============================] - 258s 4ms/step - loss: 5.3059 - categorical_accuracy: 0.3420 - val_loss: 5.8987 - val_categorical_accuracy: 0.3341\n",
            "Epoch 8/20\n",
            "61261/61261 [==============================] - 257s 4ms/step - loss: 5.1725 - categorical_accuracy: 0.3442 - val_loss: 5.9492 - val_categorical_accuracy: 0.3335\n",
            "Epoch 9/20\n",
            "61261/61261 [==============================] - 257s 4ms/step - loss: 5.0572 - categorical_accuracy: 0.3499 - val_loss: 5.9719 - val_categorical_accuracy: 0.3264\n",
            "Epoch 10/20\n",
            "61261/61261 [==============================] - 257s 4ms/step - loss: 4.9607 - categorical_accuracy: 0.3538 - val_loss: 6.0143 - val_categorical_accuracy: 0.3316\n",
            "Epoch 11/20\n",
            "61261/61261 [==============================] - 256s 4ms/step - loss: 4.8584 - categorical_accuracy: 0.3562 - val_loss: 6.1664 - val_categorical_accuracy: 0.3374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41f00f6b8cf514553e442ac6b4d356f69bdc054e",
        "id": "JwtbKF7KzeFc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ef144968-f825-48e9-fd82-55c9d47b62b1"
      },
      "cell_type": "code",
      "source": [
        "# This improved the categorical accuracy to 33 %.\n",
        "cnn_model = keras.models.load_model('model-conv1d.h5')\n",
        "metrics = cnn_model.evaluate(x_test, y_test)\n",
        "print(\"{}: {}\".format(model.metrics_names[0], metrics[0]))\n",
        "print(\"{}: {}\".format(model.metrics_names[1], metrics[1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17017/17017 [==============================] - 6s 354us/step\n",
            "loss: 0.051077511381576166\n",
            "categorical_accuracy: 0.3342539813093022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qw_juERP8412",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing the Model on New Cross Validated Questions\n",
        "We can send a request to the Stackexchange API to get a new unanswered question and list the tags associated with the question:"
      ]
    },
    {
      "metadata": {
        "id": "VZZReNyJ88so",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "b6433633-535b-4486-de5a-478aba5334a4"
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import random\n",
        "url = \"https://api.stackexchange.com/2.2/questions/unanswered?pagesize=10&order=desc&sort=votes&site=stats&filter=!-MOiNm40F1U6n0W(EFNR1)GdsWAepKpT_\"\n",
        "\n",
        "data = requests.get(url).json()\n",
        "item = random.choice(data.get('items'))\n",
        "q = item.get('title') + \" \" + strip_html_tags(item.get('body'))\n",
        "print(q)\n",
        "print(item.get('tags'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$ARIMA(p,d,q)+X_t$, Simulation over Forecasting period I have time series data and I used an $ARIMA(p,d,q)+X_t$ as the model to fit the data. The $X_t$ is an indicator random variable that is either 0 (when I don’t see a rare event) or 1 (when I see the rare event). Based on previous observations that I have for $X_t$ , I can develop a model for $X_t$ using Variable Length Markov Chain methodology. This enables me to simulate the $X_t$ over the forecasting period and gives a sequence of zeros and ones. Since this is a rare event, I will not see $X_t=1$  often. I can forecast and obtain the prediction intervals based on the simulated values for $X_t$.   \n",
            "\n",
            "Question:  \n",
            "\n",
            "How can I develop an efficient simulation procedure to take into account the occurrence of 1’s in the simulated $X_t$ over the forecasting period? I need to obtain the mean and the forecasting intervals.   \n",
            "\n",
            "The probability of observing 1 is too small for me to think that the regular Monte Carlo simulation will work well in this case. Maybe I can use “importance sampling”, but I am not sure exactly how.  \n",
            "\n",
            "Thank you.\n",
            "\n",
            "['time-series', 'forecasting', 'simulation']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aqSo6fpC8-G4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The question is: \"Simulation over Forecasting period...Thank you.\"\n",
        "\n",
        "And the author tagged the question with: ['time-series', 'forecasting', 'simulation']\n",
        "\n",
        "Now let's see which tags our models predict for the given text. We feed the question into our convolutional model and into the simple model to compare the actual tags with the computed predictions and to see how both models' predictions differ."
      ]
    },
    {
      "metadata": {
        "id": "plrJjicX9Rfj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "e39945b0-5218-41bc-951a-716c2aec3590"
      },
      "cell_type": "code",
      "source": [
        "f = get_features([q])\n",
        "p1 = prediction_to_label(cnn_model.predict(f)[0])\n",
        "p2 = prediction_to_label(simple_model.predict(f)[0])\n",
        "df = pd.DataFrame()\n",
        "df['label'] = p1.keys()\n",
        "df['p_cnn'] = p1.values()\n",
        "df['p_simple'] = df.label.apply(lambda label : p2.get(label))\n",
        "df['weighted'] = (2 * df['p_cnn'] + df['p_simple']) / 3\n",
        "df.sort_values(by='p_cnn', ascending=False)[:10]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>p_cnn</th>\n",
              "      <th>p_simple</th>\n",
              "      <th>weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>forecasting</td>\n",
              "      <td>0.577815</td>\n",
              "      <td>0.326433</td>\n",
              "      <td>0.494021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>simulation</td>\n",
              "      <td>0.568521</td>\n",
              "      <td>0.108114</td>\n",
              "      <td>0.415052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>time-series</td>\n",
              "      <td>0.353661</td>\n",
              "      <td>0.256785</td>\n",
              "      <td>0.321369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>r</td>\n",
              "      <td>0.185621</td>\n",
              "      <td>0.129774</td>\n",
              "      <td>0.167005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>monte-carlo</td>\n",
              "      <td>0.157432</td>\n",
              "      <td>0.058432</td>\n",
              "      <td>0.124432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>prediction</td>\n",
              "      <td>0.117087</td>\n",
              "      <td>0.040378</td>\n",
              "      <td>0.091517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>bootstrap</td>\n",
              "      <td>0.050716</td>\n",
              "      <td>0.002933</td>\n",
              "      <td>0.034789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>self-study</td>\n",
              "      <td>0.046530</td>\n",
              "      <td>0.005762</td>\n",
              "      <td>0.032941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>probability</td>\n",
              "      <td>0.039894</td>\n",
              "      <td>0.010047</td>\n",
              "      <td>0.029945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>mcmc</td>\n",
              "      <td>0.036926</td>\n",
              "      <td>0.176397</td>\n",
              "      <td>0.083417</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         label     p_cnn  p_simple  weighted\n",
              "0  forecasting  0.577815  0.326433  0.494021\n",
              "1   simulation  0.568521  0.108114  0.415052\n",
              "2  time-series  0.353661  0.256785  0.321369\n",
              "3            r  0.185621  0.129774  0.167005\n",
              "4  monte-carlo  0.157432  0.058432  0.124432\n",
              "5   prediction  0.117087  0.040378  0.091517\n",
              "6    bootstrap  0.050716  0.002933  0.034789\n",
              "7   self-study  0.046530  0.005762  0.032941\n",
              "8  probability  0.039894  0.010047  0.029945\n",
              "9         mcmc  0.036926  0.176397  0.083417"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "_fesWdEh9WYa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**So, The CNN model is better for estimating the 3 tags than the simple model.**\n",
        "\n",
        "There are a few noteworthy things about these results:\n",
        "\n",
        "* The first three tags are quite obvious as they are part of the question text. It is thus not a surprise that these were predicted with high confidence.\n",
        "* MCMC is an interesting prediction as it isn't mentioned in the text but fits the question well (MCMC stands for Markov Chain Monte Carlo). The ARIMA tag has a low confidence score despite being a word in the text.\n",
        "* Our simple model was able to predict the time-series tag with 23% confidence and predicts a higher confidence for r, which is also the most frequent tag in our training set. This is an indicator that our simple model is biased towards the majority class despite the class weights that we used in the training phase."
      ]
    }
  ]
}