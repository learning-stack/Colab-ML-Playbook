{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Strategy I - Processing and Understanding Text.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rqdRKxGMRmsi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Processing and Understanding Text\n",
        "## by Dipanjan (DJ) Sarkar"
      ]
    },
    {
      "metadata": {
        "id": "BcSd5V4CR3VD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72"
      ]
    },
    {
      "metadata": {
        "id": "DuAfAHZhR5-D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/NLP/Processing%20and%20Understanding%20Text/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/Processing%20and%20Understanding%20Text/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "U9JB-NSfRZX-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data retrieval"
      ]
    },
    {
      "metadata": {
        "id": "P_AIKAbfRZYB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dac4zzmGRZYI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seed_urls = ['https://inshorts.com/en/read/technology',\n",
        "             'https://inshorts.com/en/read/sports',\n",
        "             'https://inshorts.com/en/read/world']\n",
        "\n",
        "def build_dataset(seed_urls):\n",
        "    news_data = []\n",
        "    for url in seed_urls:\n",
        "        news_category = url.split('/')[-1]\n",
        "        data = requests.get(url)\n",
        "        soup = BeautifulSoup(data.content, 'html.parser')\n",
        "        \n",
        "        news_articles = [{'news_headline': headline.find('span', \n",
        "                                                         attrs={\"itemprop\": \"headline\"}).string,\n",
        "                          'news_article': article.find('div', \n",
        "                                                       attrs={\"itemprop\": \"articleBody\"}).string,\n",
        "                          'news_category': news_category}\n",
        "                         \n",
        "                            for headline, article in \n",
        "                             zip(soup.find_all('div', \n",
        "                                               class_=[\"news-card-title news-right-box\"]),\n",
        "                                 soup.find_all('div', \n",
        "                                               class_=[\"news-card-content news-right-box\"]))\n",
        "                        ]\n",
        "        news_data.extend(news_articles)\n",
        "        \n",
        "    df =  pd.DataFrame(news_data)\n",
        "    df = df[['news_headline', 'news_article', 'news_category']]\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cziL2uINRZYL",
        "colab_type": "code",
        "outputId": "0eb432e1-5505-41fe-db3c-d47306e77742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "news_df = build_dataset(seed_urls)\n",
        "news_df.head(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_headline</th>\n",
              "      <th>news_article</th>\n",
              "      <th>news_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad &amp; IIIT B'lore launch Online + Offline D...</td>\n",
              "      <td>upGrad in collaboration with IIIT Bangalore ha...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kelly Rowland says she doesn't know Excel, Mic...</td>\n",
              "      <td>American singer Kelly Rowland recently appeare...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TikTok fined $5.7 million in US over child pri...</td>\n",
              "      <td>US Federal Trade Commission has fined lip-sync...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Microsoft adds feature to import Excel data by...</td>\n",
              "      <td>Microsoft has added a new feature to the Excel...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US startup makes robot that could make door-st...</td>\n",
              "      <td>US startup Agility Robotics, a spinoff of Amer...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>TikTok downloaded more times than Instagram in...</td>\n",
              "      <td>Lip-syncing video app TikTok has surpassed one...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Facebook to launch 'clear search history' tool...</td>\n",
              "      <td>Facebook CFO David Wehner has said the social ...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Thailand passes cyber law that may enable govt...</td>\n",
              "      <td>Thailand's military-appointed Parliament has u...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Facebook sues 4 China firms for selling fake l...</td>\n",
              "      <td>Facebook has sued four firms and three people ...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Govt asks social media firms to check anti-Ind...</td>\n",
              "      <td>IT Ministry has asked social media firms to en...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       news_headline  \\\n",
              "0  upGrad & IIIT B'lore launch Online + Offline D...   \n",
              "1  Kelly Rowland says she doesn't know Excel, Mic...   \n",
              "2  TikTok fined $5.7 million in US over child pri...   \n",
              "3  Microsoft adds feature to import Excel data by...   \n",
              "4  US startup makes robot that could make door-st...   \n",
              "5  TikTok downloaded more times than Instagram in...   \n",
              "6  Facebook to launch 'clear search history' tool...   \n",
              "7  Thailand passes cyber law that may enable govt...   \n",
              "8  Facebook sues 4 China firms for selling fake l...   \n",
              "9  Govt asks social media firms to check anti-Ind...   \n",
              "\n",
              "                                        news_article news_category  \n",
              "0  upGrad in collaboration with IIIT Bangalore ha...    technology  \n",
              "1  American singer Kelly Rowland recently appeare...    technology  \n",
              "2  US Federal Trade Commission has fined lip-sync...    technology  \n",
              "3  Microsoft has added a new feature to the Excel...    technology  \n",
              "4  US startup Agility Robotics, a spinoff of Amer...    technology  \n",
              "5  Lip-syncing video app TikTok has surpassed one...    technology  \n",
              "6  Facebook CFO David Wehner has said the social ...    technology  \n",
              "7  Thailand's military-appointed Parliament has u...    technology  \n",
              "8  Facebook has sued four firms and three people ...    technology  \n",
              "9  IT Ministry has asked social media firms to en...    technology  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "5k26dDLTRZYR",
        "colab_type": "code",
        "outputId": "4ba12115-be2b-4ba2-ac52-08a75d32d2d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "news_df.news_category.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "technology    25\n",
              "sports        25\n",
              "world         24\n",
              "Name: news_category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "hB68vx0TRZYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Wrangling and Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "TlYPU7rJ0j16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "02d83de6-7ad6-4c65-a3d6-92a3369c99e1"
      },
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/52/52/a15f0fb338a462045c7c87a35dbaeda11738c45aa9d2f5c76ac191d6adff/contractions-0.0.17-py2.py3-none-any.whl\n",
            "Installing collected packages: contractions\n",
            "Successfully installed contractions-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AJAmMCbeRZYX",
        "colab_type": "code",
        "outputId": "db59e7b7-1b1a-482f-9c6e-54864061cd1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from contractions import contractions_dict\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#nlp = spacy.load('en_core', parse = True, tag=True, entity=True)\n",
        "#nlp_vec = spacy.load('en_vecs', parse = True, tag=True, entity=True)\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EHKIfNjjRZYa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove HTML tags"
      ]
    },
    {
      "metadata": {
        "id": "IlyT7a8ERZYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95b784cf-6981-4d00-8e27-ffab71c9eee1"
      },
      "cell_type": "code",
      "source": [
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text\n",
        "\n",
        "strip_html_tags('<html><h2>Some important text</h2></html>')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some important text'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "0i4BqCHERZYj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove accented characters"
      ]
    },
    {
      "metadata": {
        "id": "h2CQbc_aRZYl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5a0ba2e-5b9c-4720-9fc9-ac49ad52b096"
      },
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "remove_accented_chars('Sómě Áccěntěd těxt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some Accented text'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "pmqiX0cpRZYx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Expand contractions"
      ]
    },
    {
      "metadata": {
        "id": "YbT9n84tRZYy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b63bf4bd-e615-4492-9195-1c83db1add8c"
      },
      "cell_type": "code",
      "source": [
        "def expand_contractions(text, contraction_mapping=contractions_dict):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "\n",
        "expand_contractions(\"Y'all can't expand contractions I'd think\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You all cannot expand contractions I would think'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "oe0C5GhDRZY5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove special characters"
      ]
    },
    {
      "metadata": {
        "id": "QTyS0xhlRZY8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc8e5126-c99f-48a3-9fdf-fe7e617cf6fc"
      },
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "remove_special_characters(\"Well this was fun! What do you think? 123#@!\", \n",
        "                          remove_digits=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun What do you think '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "LH2Di22ZRZZB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text lemmatization"
      ]
    },
    {
      "metadata": {
        "id": "Q6XNz8XERZZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6c26f96-0cd2-45ad-d73f-d193449f43d0"
      },
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "\n",
        "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My system keep crash ! his crashed yesterday , ours crash daily'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "8DcKn-EzRZZM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text stemming"
      ]
    },
    {
      "metadata": {
        "id": "xPEGH_05RZZN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "564bd2fc-878d-4a5e-e9fd-1d3d44c9a756"
      },
      "cell_type": "code",
      "source": [
        "def simple_stemmer(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My system keep crash hi crash yesterday, our crash daili'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "FeCxGeksRZZd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove stopwords"
      ]
    },
    {
      "metadata": {
        "id": "CWt4eUstRZZd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e55abd11-892b-49b1-f5fc-b95d1ef1036d"
      },
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "', , stopwords , computer not'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "Y8mPWmPzRZZh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building a text normalizer"
      ]
    },
    {
      "metadata": {
        "id": "YsE3D19oRZZh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_lemmatization=True, special_char_removal=True, \n",
        "                     stopword_removal=True, remove_digits=True):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # strip HTML\n",
        "        if html_stripping:\n",
        "            doc = strip_html_tags(doc)\n",
        "        # remove accented characters\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)\n",
        "        # expand contractions    \n",
        "        if contraction_expansion:\n",
        "            doc = expand_contractions(doc)\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        # lemmatize text\n",
        "        if text_lemmatization:\n",
        "            doc = lemmatize_text(doc)\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        # remove stopwords\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "            \n",
        "        normalized_corpus.append(doc)\n",
        "        \n",
        "    return normalized_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-OutZ26RZZl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pre-process and normalize news articles"
      ]
    },
    {
      "metadata": {
        "id": "Zq_hns12RZZn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df['full_text'] = news_df[\"news_headline\"].map(str)+ '. ' + news_df[\"news_article\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O2QiDgLwRZZu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "dd28dc8f-a653-4f82-9c46-71595d62e64c"
      },
      "cell_type": "code",
      "source": [
        "news_df['clean_text'] = normalize_corpus(news_df['full_text'])\n",
        "norm_corpus = list(news_df['clean_text'])\n",
        "news_df.iloc[1][['full_text', 'clean_text']].to_dict()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clean_text': 'kelly rowland say not know excel microsoft excel respond american singer kelly rowland recently appear talk show real say not know microsoft excel not clue kelly ask excel show grammy win song dilemma video feature software respond rowland microsoft excel twitter account tweet no matter crazy',\n",
              " 'full_text': 'Kelly Rowland says she doesn\\'t know Excel, Microsoft Excel responds. American singer Kelly Rowland recently appeared on talk show \\'The Real\\', where she said, \"I don\\'t know what Microsoft Excel is...I don\\'t have a clue.\" Kelly was asked about Excel on the show as her Grammy-winning song Dilemma\\'s video featured the software. Responding to Rowland, Microsoft Excel\\'s Twitter account tweeted, \"No matter what you do, we\\'re crazy over you.\"'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "A9HmoGJFRZZx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the news articles"
      ]
    },
    {
      "metadata": {
        "id": "qdwcRlBqRZZy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df.to_csv('news.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RERpFdEARZZ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tagging Parts of Speech"
      ]
    },
    {
      "metadata": {
        "id": "YUnbn4TKRZZ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df = pd.read_csv('news.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ux2G0PNqRZZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = normalize_corpus(news_df['full_text'], text_lower_case=False, \n",
        "                          text_lemmatization=False, special_char_removal=False)\n",
        "\n",
        "sentence = str(news_df.iloc[1].news_headline)\n",
        "sentence_nlp = nlp(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aJ_eHFpjRZZ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "3877075f-bb38-45c1-a7ac-704d85ce6e98"
      },
      "cell_type": "code",
      "source": [
        "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
        "pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>POS tag</th>\n",
              "      <th>Tag type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kelly</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rowland</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>says</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>she</td>\n",
              "      <td>PRP</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>does</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>n't</td>\n",
              "      <td>RB</td>\n",
              "      <td>ADV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>know</td>\n",
              "      <td>VB</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Excel</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Microsoft</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Excel</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>responds</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Word POS tag Tag type\n",
              "0       Kelly     NNP    PROPN\n",
              "1     Rowland     NNP    PROPN\n",
              "2        says     VBZ     VERB\n",
              "3         she     PRP     PRON\n",
              "4        does     VBZ     VERB\n",
              "5         n't      RB      ADV\n",
              "6        know      VB     VERB\n",
              "7       Excel     NNP    PROPN\n",
              "8           ,       ,    PUNCT\n",
              "9   Microsoft     NNP    PROPN\n",
              "10      Excel     NNP    PROPN\n",
              "11   responds     VBZ     VERB"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "gFLPq8DcRZZ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "b55c78cd-0250-40a2-9a95-472e2525ac0b"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "nltk_pos_tagged = nltk.pos_tag(sentence.split())\n",
        "pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>POS tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kelly</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rowland</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>says</td>\n",
              "      <td>VBZ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>she</td>\n",
              "      <td>PRP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>doesn't</td>\n",
              "      <td>VBZ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>know</td>\n",
              "      <td>JJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Excel,</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Microsoft</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Excel</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>responds</td>\n",
              "      <td>NNS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Word POS tag\n",
              "0      Kelly     NNP\n",
              "1    Rowland     NNP\n",
              "2       says     VBZ\n",
              "3        she     PRP\n",
              "4    doesn't     VBZ\n",
              "5       know      JJ\n",
              "6     Excel,     NNP\n",
              "7  Microsoft     NNP\n",
              "8      Excel     NNP\n",
              "9   responds     NNS"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "k9ANGE1qRZaC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Shallow Parsing or Chunking Text"
      ]
    },
    {
      "metadata": {
        "id": "bAvge7mdRZaD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "7a631ff2-cff2-4d61-8014-bfe26ce491eb"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('conll2000')\n",
        "\n",
        "from nltk.corpus import conll2000\n",
        "data = conll2000.chunked_sents()\n",
        "\n",
        "train_data = data[:10900]\n",
        "test_data = data[10900:] \n",
        "\n",
        "print(len(train_data), len(test_data))\n",
        "print(train_data[1]) "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "10900 48\n",
            "(S\n",
            "  Chancellor/NNP\n",
            "  (PP of/IN)\n",
            "  (NP the/DT Exchequer/NNP)\n",
            "  (NP Nigel/NNP Lawson/NNP)\n",
            "  (NP 's/POS restated/VBN commitment/NN)\n",
            "  (PP to/TO)\n",
            "  (NP a/DT firm/NN monetary/JJ policy/NN)\n",
            "  (VP has/VBZ helped/VBN to/TO prevent/VB)\n",
            "  (NP a/DT freefall/NN)\n",
            "  (PP in/IN)\n",
            "  (NP sterling/NN)\n",
            "  (PP over/IN)\n",
            "  (NP the/DT past/JJ week/NN)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SqpXzInjRZaG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "c7a1cedf-101d-42fd-8c53-dbd563ea6c9a"
      },
      "cell_type": "code",
      "source": [
        "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
        "\n",
        "wtc = tree2conlltags(train_data[1])\n",
        "wtc"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Chancellor', 'NNP', 'O'),\n",
              " ('of', 'IN', 'B-PP'),\n",
              " ('the', 'DT', 'B-NP'),\n",
              " ('Exchequer', 'NNP', 'I-NP'),\n",
              " ('Nigel', 'NNP', 'B-NP'),\n",
              " ('Lawson', 'NNP', 'I-NP'),\n",
              " (\"'s\", 'POS', 'B-NP'),\n",
              " ('restated', 'VBN', 'I-NP'),\n",
              " ('commitment', 'NN', 'I-NP'),\n",
              " ('to', 'TO', 'B-PP'),\n",
              " ('a', 'DT', 'B-NP'),\n",
              " ('firm', 'NN', 'I-NP'),\n",
              " ('monetary', 'JJ', 'I-NP'),\n",
              " ('policy', 'NN', 'I-NP'),\n",
              " ('has', 'VBZ', 'B-VP'),\n",
              " ('helped', 'VBN', 'I-VP'),\n",
              " ('to', 'TO', 'I-VP'),\n",
              " ('prevent', 'VB', 'I-VP'),\n",
              " ('a', 'DT', 'B-NP'),\n",
              " ('freefall', 'NN', 'I-NP'),\n",
              " ('in', 'IN', 'B-PP'),\n",
              " ('sterling', 'NN', 'B-NP'),\n",
              " ('over', 'IN', 'B-PP'),\n",
              " ('the', 'DT', 'B-NP'),\n",
              " ('past', 'JJ', 'I-NP'),\n",
              " ('week', 'NN', 'I-NP'),\n",
              " ('.', '.', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "88CMAksIRZaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "8f33fddf-e86a-4d4f-c159-2f22d7aeda31"
      },
      "cell_type": "code",
      "source": [
        "tree = conlltags2tree(wtc) \n",
        "print(tree)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  Chancellor/NNP\n",
            "  (PP of/IN)\n",
            "  (NP the/DT Exchequer/NNP)\n",
            "  (NP Nigel/NNP Lawson/NNP)\n",
            "  (NP 's/POS restated/VBN commitment/NN)\n",
            "  (PP to/TO)\n",
            "  (NP a/DT firm/NN monetary/JJ policy/NN)\n",
            "  (VP has/VBZ helped/VBN to/TO prevent/VB)\n",
            "  (NP a/DT freefall/NN)\n",
            "  (PP in/IN)\n",
            "  (NP sterling/NN)\n",
            "  (PP over/IN)\n",
            "  (NP the/DT past/JJ week/NN)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YZU274h-RZaN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conll_tag_chunks(chunk_sents):\n",
        "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
        "    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
        "\n",
        "\n",
        "def combined_tagger(train_data, taggers, backoff=None):\n",
        "    for tagger in taggers:\n",
        "        backoff = tagger(train_data, backoff=backoff)\n",
        "    return backoff "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ib3Tz_zYRZaQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tag import UnigramTagger, BigramTagger\n",
        "from nltk.chunk import ChunkParserI\n",
        "\n",
        "class NGramTagChunker(ChunkParserI):\n",
        "    \n",
        "  def __init__(self, train_sentences, \n",
        "               tagger_classes=[UnigramTagger, BigramTagger]):\n",
        "    train_sent_tags = conll_tag_chunks(train_sentences)\n",
        "    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
        "\n",
        "  def parse(self, tagged_sentence):\n",
        "    if not tagged_sentence: \n",
        "        return None\n",
        "    pos_tags = [tag for word, tag in tagged_sentence]\n",
        "    chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
        "    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
        "    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
        "                     in zip(tagged_sentence, chunk_tags)]\n",
        "    return conlltags2tree(wpc_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vKoBMc3lRZaS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5ecaf7ab-74e4-4453-e26c-cb593a41935b"
      },
      "cell_type": "code",
      "source": [
        "ntc = NGramTagChunker(train_data)\n",
        "print(ntc.evaluate(test_data))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  90.0%%\n",
            "    Precision:     82.1%%\n",
            "    Recall:        86.3%%\n",
            "    F-Measure:     84.1%%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d_VO6b5LRZaV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "636a65b7-bc60-4155-e41c-72bb14363133"
      },
      "cell_type": "code",
      "source": [
        "chunk_tree = ntc.parse(nltk_pos_tagged)\n",
        "print(chunk_tree)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP Kelly/NNP Rowland/NNP)\n",
            "  (VP says/VBZ)\n",
            "  (NP she/PRP)\n",
            "  (VP doesn't/VBZ)\n",
            "  (NP know/JJ Excel,/NNP Microsoft/NNP Excel/NNP responds/NNS))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mh4Ll1jmRZaY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from IPython.display import display\n",
        "#os.environ['PATH'] = os.environ['PATH']+\";C:\\\\Program Files\\\\gs\\\\gs9.09\\\\bin\\\\\"\n",
        "#display(chunk_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HwnOWlIERZac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Constituency parsing"
      ]
    },
    {
      "metadata": {
        "id": "NJhPlkTmRZad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "7cb1fdef-d4ec-492d-a568-8fd4197707dc"
      },
      "cell_type": "code",
      "source": [
        "# set java path\n",
        "import os\n",
        "java_path = r'C:\\Program Files\\Java\\jdk1.8.0_102\\bin\\java.exe'\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "\n",
        "from nltk.parse.stanford import StanfordParser\n",
        "\n",
        "scp = StanfordParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
        "                   path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
        "                   \n",
        "result = list(scp.raw_parse(sentence))\n",
        "print(result[0])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-01564b79acea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m scp = StanfordParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n\u001b[0;32m----> 8\u001b[0;31m                    path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             ),\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 637\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-parser\\.jar jar file at E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "o_NF1XT0RZag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "os.environ['PATH'] = os.environ['PATH']+\";C:\\\\Program Files\\\\gs\\\\gs9.09\\\\bin\\\\\"\n",
        "display(result[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3W1u9w9CRZaj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dependency parsing"
      ]
    },
    {
      "metadata": {
        "id": "QelU4TWmRZak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dependency_pattern = '{left}<---{word}[{w_type}]--->{right}\\n--------'\n",
        "for token in sentence_nlp:\n",
        "    print(dependency_pattern.format(word=token.orth_, \n",
        "                                  w_type=token.dep_,\n",
        "                                  left=[t.orth_ \n",
        "                                            for t \n",
        "                                            in token.lefts],\n",
        "                                  right=[t.orth_ \n",
        "                                             for t \n",
        "                                             in token.rights]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OommzdamRZan",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(sentence_nlp, jupyter=True, \n",
        "                options={'distance': 110,\n",
        "                         'arrow_stroke': 2,\n",
        "                         'arrow_width': 8})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H29DQ5hsRZax",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.parse.stanford import StanfordDependencyParser\n",
        "sdp = StanfordDependencyParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
        "                               path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')    \n",
        "result = list(sdp.raw_parse(sentence))  \n",
        "dep_tree = [parse.tree() for parse in result][0]\n",
        "print(dep_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cf3pnWrPRZa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "os.environ['PATH'] = os.environ['PATH']+\";C:\\\\Program Files\\\\gs\\\\gs9.09\\\\bin\\\\\"\n",
        "display(dep_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gYXLloMRZa_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from graphviz import Source\n",
        "\n",
        "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
        "source = Source(dep_tree_dot_repr, filename=\"dep_tree\", format=\"png\")\n",
        "source"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PCz7lyfsRZbD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition"
      ]
    },
    {
      "metadata": {
        "id": "NG5ZZu1ORZbD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence = str(news_df.iloc[1].full_text)\n",
        "sentence_nlp = nlp(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ih9foahdRZbF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print([(word, word.ent_type_) for word in sentence_nlp if word.ent_type_])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cX2206aZRZbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "displacy.render(sentence_nlp, style='ent', jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JVMgLoJfRZbT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "named_entities = []\n",
        "for sentence in corpus:\n",
        "    temp_entity_name = ''\n",
        "    temp_named_entity = None\n",
        "    sentence = nlp(sentence)\n",
        "    for word in sentence:\n",
        "        term = word.text \n",
        "        tag = word.ent_type_\n",
        "        if tag:\n",
        "            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
        "            temp_named_entity = (temp_entity_name, tag)\n",
        "        else:\n",
        "            if temp_named_entity:\n",
        "                named_entities.append(temp_named_entity)\n",
        "                temp_entity_name = ''\n",
        "                temp_named_entity = None\n",
        "\n",
        "entity_frame = pd.DataFrame(named_entities, \n",
        "                            columns=['Entity Name', 'Entity Type'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0MNabBGZRZbV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n",
        "                           .size()\n",
        "                           .sort_values(ascending=False)\n",
        "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
        "top_entities.T.iloc[:,:15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eRZ2kX6CRZbZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_entities = (entity_frame.groupby(by=['Entity Type'])\n",
        "                           .size()\n",
        "                           .sort_values(ascending=False)\n",
        "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
        "top_entities.T.iloc[:,:15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C-BE6EmYRZbc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tag import StanfordNERTagger\n",
        "import os\n",
        "\n",
        "java_path = r'C:\\Program Files\\Java\\jdk1.8.0_102\\bin\\java.exe'\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "\n",
        "sn = StanfordNERTagger('E:/stanford/stanford-ner-2014-08-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
        "                       path_to_jar='E:/stanford/stanford-ner-2014-08-27/stanford-ner.jar')\n",
        "\n",
        "ner_tagged_sentences = [sn.tag(sent.split()) for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2tpoCQdjRZbf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "named_entities = []\n",
        "for sentence in ner_tagged_sentences:\n",
        "    temp_entity_name = ''\n",
        "    temp_named_entity = None\n",
        "    for term, tag in sentence:\n",
        "        if tag != 'O':\n",
        "            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
        "            temp_named_entity = (temp_entity_name, tag)\n",
        "        else:\n",
        "            if temp_named_entity:\n",
        "                named_entities.append(temp_named_entity)\n",
        "                temp_entity_name = ''\n",
        "                temp_named_entity = None\n",
        "\n",
        "#named_entities = list(set(named_entities))\n",
        "entity_frame = pd.DataFrame(named_entities, \n",
        "                            columns=['Entity Name', 'Entity Type'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYZw_W6aRZbg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n",
        "                           .size()\n",
        "                           .sort_values(ascending=False)\n",
        "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
        "top_entities.head(15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "epy1WSwIRZbj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_entities = (entity_frame.groupby(by=['Entity Type'])\n",
        "                           .size()\n",
        "                           .sort_values(ascending=False)\n",
        "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
        "top_entities.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i0ADAWnPRZbl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Emotion and Sentiment Analysis"
      ]
    },
    {
      "metadata": {
        "id": "7phvrTrFRZbl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from afinn import Afinn\n",
        "\n",
        "af = Afinn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xTFoWbXWRZbo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentiment_scores = [af.score(article) for article in corpus]\n",
        "sentiment_category = ['positive' if score > 0 \n",
        "                          else 'negative' if score < 0 \n",
        "                              else 'neutral' \n",
        "                                  for score in sentiment_scores]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EMDVEHkVRZbq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([list(news_df['news_category']), sentiment_scores, sentiment_category]).T\n",
        "df.columns = ['news_category', 'sentiment_score', 'sentiment_category']\n",
        "df['sentiment_score'] = df.sentiment_score.astype('float')\n",
        "df.groupby(by=['news_category']).describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x5O0V0cgRZbt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
        "sp = sns.stripplot(x='news_category', y=\"sentiment_score\", \n",
        "                   hue='news_category', data=df, ax=ax1)\n",
        "bp = sns.boxplot(x='news_category', y=\"sentiment_score\", \n",
        "                 hue='news_category', data=df, palette=\"Set2\", ax=ax2)\n",
        "t = f.suptitle('Visualizing News Sentiment', fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ldfjhc-nRZbx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fc = sns.factorplot(x=\"news_category\", hue=\"sentiment_category\", \n",
        "                    data=df, kind=\"count\", \n",
        "                    palette={\"negative\": \"#FE2020\", \n",
        "                             \"positive\": \"#BADD07\", \n",
        "                             \"neutral\": \"#68BFF5\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hoaa9AYLRZb0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pos_idx = df[(df.news_category=='technology') & (df.sentiment_score == 6)].index[0]\n",
        "neg_idx = df[(df.news_category=='technology') & (df.sentiment_score == -15)].index[0]\n",
        "\n",
        "print('Most Negative Tech News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
        "print()\n",
        "print('Most Positive Tech News Article:', news_df.iloc[pos_idx][['news_article']][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wy34vD3aRZb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pos_idx = df[(df.news_category=='world') & (df.sentiment_score == 16)].index[0]\n",
        "neg_idx = df[(df.news_category=='world') & (df.sentiment_score == -12)].index[0]\n",
        "\n",
        "print('Most Negative World News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
        "print()\n",
        "print('Most Positive World News Article:', news_df.iloc[pos_idx][['news_article']][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quE0IcANRZb5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "sentiment_scores_tb = [round(TextBlob(article).sentiment.polarity, 3) for article in news_df['clean_text']]\n",
        "sentiment_category_tb = ['positive' if score > 0 \n",
        "                             else 'negative' if score < 0 \n",
        "                                 else 'neutral' \n",
        "                                     for score in sentiment_scores_tb]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ETDPX8xZRZb-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([list(news_df['news_category']), sentiment_scores_tb, sentiment_category_tb]).T\n",
        "df.columns = ['news_category', 'sentiment_score', 'sentiment_category']\n",
        "df['sentiment_score'] = df.sentiment_score.astype('float')\n",
        "df.groupby(by=['news_category']).describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15CFTHyxRZcB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y6dOv9AzRZcE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fc = sns.factorplot(x=\"news_category\", hue=\"sentiment_category\", \n",
        "                    data=df, kind=\"count\", \n",
        "                    palette={\"negative\": \"#FE2020\", \n",
        "                             \"positive\": \"#BADD07\", \n",
        "                             \"neutral\": \"#68BFF5\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ULlq4yjRZcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pos_idx = df[(df.news_category=='world') & (df.sentiment_score == 0.7)].index[0]\n",
        "neg_idx = df[(df.news_category=='world') & (df.sentiment_score == -0.296)].index[0]\n",
        "\n",
        "print('Most Negative World News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
        "print()\n",
        "print('Most Positive World News Article:', news_df.iloc[pos_idx][['news_article']][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GCN68R_HRZcH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import model_evaluation_utils as meu\n",
        "meu.display_confusion_matrix_pretty(true_labels=sentiment_category, \n",
        "                                    predicted_labels=sentiment_category_tb, \n",
        "                                    classes=['negative', 'neutral', 'positive'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}