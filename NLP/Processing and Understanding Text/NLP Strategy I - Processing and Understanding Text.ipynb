{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Strategy I - Processing and Understanding Text.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rqdRKxGMRmsi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Processing and Understanding Text\n",
        "## by Dipanjan (DJ) Sarkar"
      ]
    },
    {
      "metadata": {
        "id": "BcSd5V4CR3VD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72"
      ]
    },
    {
      "metadata": {
        "id": "DuAfAHZhR5-D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/NLP/Processing%20and%20Understanding%20Text/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/Processing%20and%20Understanding%20Text/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "U9JB-NSfRZX-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data retrieval"
      ]
    },
    {
      "metadata": {
        "id": "P_AIKAbfRZYB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dac4zzmGRZYI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seed_urls = ['https://inshorts.com/en/read/technology',\n",
        "             'https://inshorts.com/en/read/sports',\n",
        "             'https://inshorts.com/en/read/world']\n",
        "\n",
        "def build_dataset(seed_urls):\n",
        "    news_data = []\n",
        "    for url in seed_urls:\n",
        "        news_category = url.split('/')[-1]\n",
        "        data = requests.get(url)\n",
        "        soup = BeautifulSoup(data.content, 'html.parser')\n",
        "        \n",
        "        news_articles = [{'news_headline': headline.find('span', \n",
        "                                                         attrs={\"itemprop\": \"headline\"}).string,\n",
        "                          'news_article': article.find('div', \n",
        "                                                       attrs={\"itemprop\": \"articleBody\"}).string,\n",
        "                          'news_category': news_category}\n",
        "                         \n",
        "                            for headline, article in \n",
        "                             zip(soup.find_all('div', \n",
        "                                               class_=[\"news-card-title news-right-box\"]),\n",
        "                                 soup.find_all('div', \n",
        "                                               class_=[\"news-card-content news-right-box\"]))\n",
        "                        ]\n",
        "        news_data.extend(news_articles)\n",
        "        \n",
        "    df =  pd.DataFrame(news_data)\n",
        "    df = df[['news_headline', 'news_article', 'news_category']]\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cziL2uINRZYL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df = build_dataset(seed_urls)\n",
        "news_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5k26dDLTRZYR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df.news_category.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hB68vx0TRZYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Wrangling and Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "AJAmMCbeRZYX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from contractions import CONTRACTION_MAP\n",
        "import unicodedata\n",
        "\n",
        "nlp = spacy.load('en_core', parse = True, tag=True, entity=True)\n",
        "#nlp_vec = spacy.load('en_vecs', parse = True, tag=True, entity=True)\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EHKIfNjjRZYa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove HTML tags"
      ]
    },
    {
      "metadata": {
        "id": "IlyT7a8ERZYb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text\n",
        "\n",
        "strip_html_tags('<html><h2>Some important text</h2></html>')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0i4BqCHERZYj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove accented characters"
      ]
    },
    {
      "metadata": {
        "id": "h2CQbc_aRZYl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "remove_accented_chars('Sómě Áccěntěd těxt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pmqiX0cpRZYx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Expand contractions"
      ]
    },
    {
      "metadata": {
        "id": "YbT9n84tRZYy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "\n",
        "expand_contractions(\"Y'all can't expand contractions I'd think\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oe0C5GhDRZY5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove special characters"
      ]
    },
    {
      "metadata": {
        "id": "QTyS0xhlRZY8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "remove_special_characters(\"Well this was fun! What do you think? 123#@!\", \n",
        "                          remove_digits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LH2Di22ZRZZB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text lemmatization"
      ]
    },
    {
      "metadata": {
        "id": "Q6XNz8XERZZC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "\n",
        "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8DcKn-EzRZZM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text stemming"
      ]
    },
    {
      "metadata": {
        "id": "xPEGH_05RZZN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def simple_stemmer(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FeCxGeksRZZd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove stopwords"
      ]
    },
    {
      "metadata": {
        "id": "CWt4eUstRZZd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y8mPWmPzRZZh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building a text normalizer"
      ]
    },
    {
      "metadata": {
        "id": "YsE3D19oRZZh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_lemmatization=True, special_char_removal=True, \n",
        "                     stopword_removal=True, remove_digits=True):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # strip HTML\n",
        "        if html_stripping:\n",
        "            doc = strip_html_tags(doc)\n",
        "        # remove accented characters\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)\n",
        "        # expand contractions    \n",
        "        if contraction_expansion:\n",
        "            doc = expand_contractions(doc)\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        # lemmatize text\n",
        "        if text_lemmatization:\n",
        "            doc = lemmatize_text(doc)\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        # remove stopwords\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "            \n",
        "        normalized_corpus.append(doc)\n",
        "        \n",
        "    return normalized_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-OutZ26RZZl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pre-process and normalize news articles"
      ]
    },
    {
      "metadata": {
        "id": "Zq_hns12RZZn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df['full_text'] = news_df[\"news_headline\"].map(str)+ '. ' + news_df[\"news_article\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O2QiDgLwRZZu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df['clean_text'] = normalize_corpus(news_df['full_text'])\n",
        "norm_corpus = list(news_df['clean_text'])\n",
        "news_df.iloc[1][['full_text', 'clean_text']].to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A9HmoGJFRZZx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the news articles"
      ]
    },
    {
      "metadata": {
        "id": "qdwcRlBqRZZy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df.to_csv('news.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RERpFdEARZZ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tagging Parts of Speech"
      ]
    },
    {
      "metadata": {
        "id": "YUnbn4TKRZZ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df = pd.read_csv('news.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ux2G0PNqRZZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = normalize_corpus(news_df['full_text'], text_lower_case=False, \n",
        "                          text_lemmatization=False, special_char_removal=False)\n",
        "\n",
        "sentence = str(news_df.iloc[1].news_headline)\n",
        "sentence_nlp = nlp(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aJ_eHFpjRZZ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
        "pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gFLPq8DcRZZ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nltk_pos_tagged = nltk.pos_tag(sentence.split())\n",
        "pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k9ANGE1qRZaC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Shallow Parsing or Chunking Text"
      ]
    },
    {
      "metadata": {
        "id": "bAvge7mdRZaD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import conll2000\n",
        "data = conll2000.chunked_sents()\n",
        "\n",
        "train_data = data[:10900]\n",
        "test_data = data[10900:] \n",
        "\n",
        "print(len(train_data), len(test_data))\n",
        "print(train_data[1]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SqpXzInjRZaG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
        "\n",
        "wtc = tree2conlltags(train_data[1])\n",
        "wtc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "88CMAksIRZaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tree = conlltags2tree(wtc) \n",
        "print(tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YZU274h-RZaN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conll_tag_chunks(chunk_sents):\n",
        "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
        "    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
        "\n",
        "\n",
        "def combined_tagger(train_data, taggers, backoff=None):\n",
        "    for tagger in taggers:\n",
        "        backoff = tagger(train_data, backoff=backoff)\n",
        "    return backoff "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ib3Tz_zYRZaQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tag import UnigramTagger, BigramTagger\n",
        "from nltk.chunk import ChunkParserI\n",
        "\n",
        "class NGramTagChunker(ChunkParserI):\n",
        "    \n",
        "  def __init__(self, train_sentences, \n",
        "               tagger_classes=[UnigramTagger, BigramTagger]):\n",
        "    train_sent_tags = conll_tag_chunks(train_sentences)\n",
        "    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
        "\n",
        "  def parse(self, tagged_sentence):\n",
        "    if not tagged_sentence: \n",
        "        return None\n",
        "    pos_tags = [tag for word, tag in tagged_sentence]\n",
        "    chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
        "    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
        "    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
        "                     in zip(tagged_sentence, chunk_tags)]\n",
        "    return conlltags2tree(wpc_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vKoBMc3lRZaS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ntc = NGramTagChunker(train_data)\n",
        "print(ntc.evaluate(test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d_VO6b5LRZaV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "chunk_tree = ntc.parse(nltk_pos_tagged)\n",
        "print(chunk_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mh4Ll1jmRZaY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "os.environ['PATH'] = os.environ['PATH']+\";C:\\\\Program Files\\\\gs\\\\gs9.09\\\\bin\\\\\"\n",
        "display(chunk_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HwnOWlIERZac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Constituency parsing"
      ]
    },
    {
      "metadata": {
        "id": "NJhPlkTmRZad",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set java path\n",
        "import os\n",
        "java_path = r'C:\\Program Files\\Java\\jdk1.8.0_102\\bin\\java.exe'\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "\n",
        "from nltk.parse.stanford import StanfordParser\n",
        "\n",
        "scp = StanfordParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
        "                   path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
        "                   \n",
        "result = list(scp.raw_parse(sentence))\n",
        "print(result[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o_NF1XT0RZag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "os.environ['PATH'] = os.environ['PATH']+\";C:\\\\Program Files\\\\gs\\\\gs9.09\\\\bin\\\\\"\n",
        "display(result[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3W1u9w9CRZaj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dependency parsing"
      ]
    },
    {
      "metadata": {
        "id": "QelU4TWmRZak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dependency_pattern = '{left}<---{word}[{w_type}]--->{right}\\n--------'\n",
        "for token in sentence_nlp:\n",
        "    print(dependency_pattern.format(word=token.orth_, \n",
        "                                  w_type=token.dep_,\n",
        "                                  left=[t.orth_ \n",
        "                                            for t \n",
        "                                            in token.lefts],\n",
        "                                  right=[t.orth_ \n",
        "                                             for t \n",
        "                                             in token.rights]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OommzdamRZan",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(sentence_nlp, jupyter=True, \n",
        "                options={'distance': 110,\n",
        "                         'arrow_stroke': 2,\n",
        "                         'arrow_width': 8})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H29DQ5hsRZax",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.parse.stanford import StanfordDependencyParser\n",
        "sdp = StanfordDependencyParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
        "                               path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')    \n",
        "result = list(sdp.raw_parse(sentence))  \n",
        "dep_tree = [parse.tree() for parse in result][0]\n",
        "print(dep_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cf3pnWrPRZa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "os.environ['PATH'] = os.environ['PATH']+\";C:\\\\Program Files\\\\gs\\\\gs9.09\\\\bin\\\\\"\n",
        "display(dep_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gYXLloMRZa_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from graphviz import Source\n",
        "\n",
        "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
        "source = Source(dep_tree_dot_repr, filename=\"dep_tree\", format=\"png\")\n",
        "source"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PCz7lyfsRZbD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition"
      ]
    },
    {
      "metadata": {
        "id": "NG5ZZu1ORZbD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence = str(news_df.iloc[1].full_text)\n",
        "sentence_nlp = nlp(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ih9foahdRZbF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print([(word, word.ent_type_) for word in sentence_nlp if word.ent_type_])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cX2206aZRZbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "displacy.render(sentence_nlp, style='ent', jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JVMgLoJfRZbT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "named_entities = []\n",
        "for sentence in corpus:\n",
        "    temp_entity_name = ''\n",
        "    temp_named_entity = None\n",
        "    sentence = nlp(sentence)\n",
        "    for word in sentence:\n",
        "        term = word.text \n",
        "        tag = word.ent_type_\n",
        "        if tag:\n",
        "            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
        "            temp_named_entity = (temp_entity_name, tag)\n",
        "        else:\n",
        "            if temp_named_entity:\n",
        "                named_entities.append(temp_named_entity)\n",
        "                temp_entity_name = ''\n",
        "                temp_named_entity = None\n",
        "\n",
        "entity_frame = pd.DataFrame(named_entities, \n",
        "                            columns=['Entity Name', 'Entity Type'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0MNabBGZRZbV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n",
        "                           .size()\n",
        "                           .sort_values(ascending=False)\n",
        "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
        "top_entities.T.iloc[:,:15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eRZ2kX6CRZbZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_entities = (entity_frame.groupby(by=['Entity Type'])\n",
        "                           .size()\n",
        "                           .sort_values(ascending=False)\n",
        "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
        "top_entities.T.iloc[:,:15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C-BE6EmYRZbc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tag import StanfordNERTagger\n",
        "import os\n",
        "\n",
        "java_path = r'C:\\Program Files\\Java\\jdk1.8.0_102\\bin\\java.exe'\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "\n",
        "sn = StanfordNERTagger('E:/stanford/stanford-ner-2014-08-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
        "                       path_to_jar='E:/stanford/stanford-ner-2014-08-27/stanford-ner.jar')\n",
        "\n",
        "ner_tagged_sentences = [sn.tag(sent.split()) for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2tpoCQdjRZbf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "named_entities = []\n",
        "for sentence in ner_tagged_sentences:\n",
        "    temp_entity_name = ''\n",
        "    temp_named_entity = None\n",
        "    for term, tag in sentence:\n",
        "        if tag != 'O':\n",
        "            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
        "            temp_named_entity = (temp_entity_name, tag)\n",
        "        else:\n",
        "            if temp_named_entity:\n",
        "                named_entities.append(temp_named_entity)\n",
        "                temp_entity_name = ''\n",
        "                temp_named_entity = None\n",
        "\n",
        "#named_entities = list(set(named_entities))\n",
        "entity_frame = pd.DataFrame(named_entities, \n",
        "                            columns=['Entity Name', 'Entity Type'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYZw_W6aRZbg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n",
        "                           .size()\n",
        "                           .sort_values(ascending=False)\n",
        "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
        "top_entities.head(15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "epy1WSwIRZbj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_entities = (entity_frame.groupby(by=['Entity Type'])\n",
        "                           .size()\n",
        "                           .sort_values(ascending=False)\n",
        "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
        "top_entities.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i0ADAWnPRZbl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Emotion and Sentiment Analysis"
      ]
    },
    {
      "metadata": {
        "id": "7phvrTrFRZbl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from afinn import Afinn\n",
        "\n",
        "af = Afinn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xTFoWbXWRZbo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentiment_scores = [af.score(article) for article in corpus]\n",
        "sentiment_category = ['positive' if score > 0 \n",
        "                          else 'negative' if score < 0 \n",
        "                              else 'neutral' \n",
        "                                  for score in sentiment_scores]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EMDVEHkVRZbq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([list(news_df['news_category']), sentiment_scores, sentiment_category]).T\n",
        "df.columns = ['news_category', 'sentiment_score', 'sentiment_category']\n",
        "df['sentiment_score'] = df.sentiment_score.astype('float')\n",
        "df.groupby(by=['news_category']).describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x5O0V0cgRZbt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
        "sp = sns.stripplot(x='news_category', y=\"sentiment_score\", \n",
        "                   hue='news_category', data=df, ax=ax1)\n",
        "bp = sns.boxplot(x='news_category', y=\"sentiment_score\", \n",
        "                 hue='news_category', data=df, palette=\"Set2\", ax=ax2)\n",
        "t = f.suptitle('Visualizing News Sentiment', fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ldfjhc-nRZbx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fc = sns.factorplot(x=\"news_category\", hue=\"sentiment_category\", \n",
        "                    data=df, kind=\"count\", \n",
        "                    palette={\"negative\": \"#FE2020\", \n",
        "                             \"positive\": \"#BADD07\", \n",
        "                             \"neutral\": \"#68BFF5\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hoaa9AYLRZb0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pos_idx = df[(df.news_category=='technology') & (df.sentiment_score == 6)].index[0]\n",
        "neg_idx = df[(df.news_category=='technology') & (df.sentiment_score == -15)].index[0]\n",
        "\n",
        "print('Most Negative Tech News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
        "print()\n",
        "print('Most Positive Tech News Article:', news_df.iloc[pos_idx][['news_article']][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wy34vD3aRZb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pos_idx = df[(df.news_category=='world') & (df.sentiment_score == 16)].index[0]\n",
        "neg_idx = df[(df.news_category=='world') & (df.sentiment_score == -12)].index[0]\n",
        "\n",
        "print('Most Negative World News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
        "print()\n",
        "print('Most Positive World News Article:', news_df.iloc[pos_idx][['news_article']][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quE0IcANRZb5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "sentiment_scores_tb = [round(TextBlob(article).sentiment.polarity, 3) for article in news_df['clean_text']]\n",
        "sentiment_category_tb = ['positive' if score > 0 \n",
        "                             else 'negative' if score < 0 \n",
        "                                 else 'neutral' \n",
        "                                     for score in sentiment_scores_tb]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ETDPX8xZRZb-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([list(news_df['news_category']), sentiment_scores_tb, sentiment_category_tb]).T\n",
        "df.columns = ['news_category', 'sentiment_score', 'sentiment_category']\n",
        "df['sentiment_score'] = df.sentiment_score.astype('float')\n",
        "df.groupby(by=['news_category']).describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15CFTHyxRZcB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y6dOv9AzRZcE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fc = sns.factorplot(x=\"news_category\", hue=\"sentiment_category\", \n",
        "                    data=df, kind=\"count\", \n",
        "                    palette={\"negative\": \"#FE2020\", \n",
        "                             \"positive\": \"#BADD07\", \n",
        "                             \"neutral\": \"#68BFF5\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ULlq4yjRZcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pos_idx = df[(df.news_category=='world') & (df.sentiment_score == 0.7)].index[0]\n",
        "neg_idx = df[(df.news_category=='world') & (df.sentiment_score == -0.296)].index[0]\n",
        "\n",
        "print('Most Negative World News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
        "print()\n",
        "print('Most Positive World News Article:', news_df.iloc[pos_idx][['news_article']][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GCN68R_HRZcH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import model_evaluation_utils as meu\n",
        "meu.display_confusion_matrix_pretty(true_labels=sentiment_category, \n",
        "                                    predicted_labels=sentiment_category_tb, \n",
        "                                    classes=['negative', 'neutral', 'positive'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}