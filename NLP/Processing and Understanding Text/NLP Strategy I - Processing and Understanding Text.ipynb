{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Strategy I - Processing and Understanding Text.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rqdRKxGMRmsi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Processing and Understanding Text\n",
        "## by Dipanjan (DJ) Sarkar"
      ]
    },
    {
      "metadata": {
        "id": "BcSd5V4CR3VD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72"
      ]
    },
    {
      "metadata": {
        "id": "DuAfAHZhR5-D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/NLP/Processing%20and%20Understanding%20Text/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/Processing%20and%20Understanding%20Text/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "U9JB-NSfRZX-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data retrieval"
      ]
    },
    {
      "metadata": {
        "id": "P_AIKAbfRZYB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dac4zzmGRZYI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seed_urls = ['https://inshorts.com/en/read/technology',\n",
        "             'https://inshorts.com/en/read/sports',\n",
        "             'https://inshorts.com/en/read/world']\n",
        "\n",
        "def build_dataset(seed_urls):\n",
        "    news_data = []\n",
        "    for url in seed_urls:\n",
        "        news_category = url.split('/')[-1]\n",
        "        data = requests.get(url)\n",
        "        soup = BeautifulSoup(data.content, 'html.parser')\n",
        "        \n",
        "        news_articles = [{'news_headline': headline.find('span', \n",
        "                                                         attrs={\"itemprop\": \"headline\"}).string,\n",
        "                          'news_article': article.find('div', \n",
        "                                                       attrs={\"itemprop\": \"articleBody\"}).string,\n",
        "                          'news_category': news_category}\n",
        "                         \n",
        "                            for headline, article in \n",
        "                             zip(soup.find_all('div', \n",
        "                                               class_=[\"news-card-title news-right-box\"]),\n",
        "                                 soup.find_all('div', \n",
        "                                               class_=[\"news-card-content news-right-box\"]))\n",
        "                        ]\n",
        "        news_data.extend(news_articles)\n",
        "        \n",
        "    df =  pd.DataFrame(news_data)\n",
        "    df = df[['news_headline', 'news_article', 'news_category']]\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cziL2uINRZYL",
        "colab_type": "code",
        "outputId": "f25d4579-3d93-4032-a19a-1c05ab88e503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "news_df = build_dataset(seed_urls)\n",
        "news_df.head(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_headline</th>\n",
              "      <th>news_article</th>\n",
              "      <th>news_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad &amp; IIIT B'lore launch Online + Offline D...</td>\n",
              "      <td>upGrad in collaboration with IIIT Bangalore ha...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kelly Rowland says she doesn't know Excel, Mic...</td>\n",
              "      <td>American singer Kelly Rowland recently appeare...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Microsoft adds feature to import Excel data by...</td>\n",
              "      <td>Microsoft has added a new feature to the Excel...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TikTok fined $5.7 million in US over child pri...</td>\n",
              "      <td>US Federal Trade Commission has fined lip-sync...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US startup makes robot that could make door-st...</td>\n",
              "      <td>US startup Agility Robotics, a spinoff of Amer...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Boeing unveils AI-powered unmanned fighter jet...</td>\n",
              "      <td>Aircraft manufacturer Boeing has unveiled an a...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Apple made 50% world smartwatch shipments in Q...</td>\n",
              "      <td>Apple Watch consisted of over 50% (around 9.2 ...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>YouTube disables comments on some videos over ...</td>\n",
              "      <td>YouTube has disabled comments on some videos f...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Amazon smart doorbell bug let fake videos be p...</td>\n",
              "      <td>Cybersecurity company Dojo's researchers demoe...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Samsung, Huawei agree to settle 2-year-old US ...</td>\n",
              "      <td>Samsung and Huawei have filed a joint motion i...</td>\n",
              "      <td>technology</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       news_headline  \\\n",
              "0  upGrad & IIIT B'lore launch Online + Offline D...   \n",
              "1  Kelly Rowland says she doesn't know Excel, Mic...   \n",
              "2  Microsoft adds feature to import Excel data by...   \n",
              "3  TikTok fined $5.7 million in US over child pri...   \n",
              "4  US startup makes robot that could make door-st...   \n",
              "5  Boeing unveils AI-powered unmanned fighter jet...   \n",
              "6  Apple made 50% world smartwatch shipments in Q...   \n",
              "7  YouTube disables comments on some videos over ...   \n",
              "8  Amazon smart doorbell bug let fake videos be p...   \n",
              "9  Samsung, Huawei agree to settle 2-year-old US ...   \n",
              "\n",
              "                                        news_article news_category  \n",
              "0  upGrad in collaboration with IIIT Bangalore ha...    technology  \n",
              "1  American singer Kelly Rowland recently appeare...    technology  \n",
              "2  Microsoft has added a new feature to the Excel...    technology  \n",
              "3  US Federal Trade Commission has fined lip-sync...    technology  \n",
              "4  US startup Agility Robotics, a spinoff of Amer...    technology  \n",
              "5  Aircraft manufacturer Boeing has unveiled an a...    technology  \n",
              "6  Apple Watch consisted of over 50% (around 9.2 ...    technology  \n",
              "7  YouTube has disabled comments on some videos f...    technology  \n",
              "8  Cybersecurity company Dojo's researchers demoe...    technology  \n",
              "9  Samsung and Huawei have filed a joint motion i...    technology  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "5k26dDLTRZYR",
        "colab_type": "code",
        "outputId": "d31f51a6-7c5b-4502-a704-1edb05e28e42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "news_df.news_category.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "technology    25\n",
              "sports        25\n",
              "world         24\n",
              "Name: news_category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "hB68vx0TRZYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Wrangling and Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "TlYPU7rJ0j16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "39a88fe4-b349-496a-a082-2a85a246a719"
      },
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/52/52/a15f0fb338a462045c7c87a35dbaeda11738c45aa9d2f5c76ac191d6adff/contractions-0.0.17-py2.py3-none-any.whl\n",
            "Installing collected packages: contractions\n",
            "Successfully installed contractions-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AJAmMCbeRZYX",
        "colab_type": "code",
        "outputId": "ffc5b304-94eb-47ac-87a2-3344f4017fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from contractions import contractions_dict\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#nlp = spacy.load('en_core', parse = True, tag=True, entity=True)\n",
        "#nlp_vec = spacy.load('en_vecs', parse = True, tag=True, entity=True)\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EHKIfNjjRZYa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove HTML tags"
      ]
    },
    {
      "metadata": {
        "id": "IlyT7a8ERZYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a06242df-15a0-42f1-9b7d-e8bb3e1edb6e"
      },
      "cell_type": "code",
      "source": [
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text\n",
        "\n",
        "strip_html_tags('<html><h2>Some important text</h2></html>')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some important text'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "0i4BqCHERZYj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove accented characters"
      ]
    },
    {
      "metadata": {
        "id": "h2CQbc_aRZYl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b21d8bc9-57e2-4f3b-8cc5-b40acc75c40b"
      },
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "remove_accented_chars('Sómě Áccěntěd těxt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some Accented text'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "pmqiX0cpRZYx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Expand contractions"
      ]
    },
    {
      "metadata": {
        "id": "YbT9n84tRZYy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59dd697b-d2dd-4f48-a6d3-d3e82e0516ab"
      },
      "cell_type": "code",
      "source": [
        "def expand_contractions(text, contraction_mapping=contractions_dict):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "\n",
        "expand_contractions(\"Y'all can't expand contractions I'd think\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You all cannot expand contractions I would think'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "oe0C5GhDRZY5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove special characters"
      ]
    },
    {
      "metadata": {
        "id": "QTyS0xhlRZY8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b603ddf-d2e2-4d76-96ed-be756ef568fa"
      },
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "remove_special_characters(\"Well this was fun! What do you think? 123#@!\", \n",
        "                          remove_digits=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun What do you think '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "LH2Di22ZRZZB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text lemmatization"
      ]
    },
    {
      "metadata": {
        "id": "Q6XNz8XERZZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c364ecc-e2c7-49f8-b9a5-52be6d8a0389"
      },
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "\n",
        "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My system keep crash ! his crashed yesterday , ours crash daily'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "8DcKn-EzRZZM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text stemming"
      ]
    },
    {
      "metadata": {
        "id": "xPEGH_05RZZN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68a23501-6fe1-4d11-9e6d-21675e77ff1a"
      },
      "cell_type": "code",
      "source": [
        "def simple_stemmer(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My system keep crash hi crash yesterday, our crash daili'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "FeCxGeksRZZd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove stopwords"
      ]
    },
    {
      "metadata": {
        "id": "CWt4eUstRZZd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e04b0b6b-52df-429b-b2c6-796198fe6ebd"
      },
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "', , stopwords , computer not'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "Y8mPWmPzRZZh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building a text normalizer"
      ]
    },
    {
      "metadata": {
        "id": "YsE3D19oRZZh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_lemmatization=True, special_char_removal=True, \n",
        "                     stopword_removal=True, remove_digits=True):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # strip HTML\n",
        "        if html_stripping:\n",
        "            doc = strip_html_tags(doc)\n",
        "        # remove accented characters\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)\n",
        "        # expand contractions    \n",
        "        if contraction_expansion:\n",
        "            doc = expand_contractions(doc)\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        # lemmatize text\n",
        "        if text_lemmatization:\n",
        "            doc = lemmatize_text(doc)\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        # remove stopwords\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "            \n",
        "        normalized_corpus.append(doc)\n",
        "        \n",
        "    return normalized_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-OutZ26RZZl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pre-process and normalize news articles"
      ]
    },
    {
      "metadata": {
        "id": "Zq_hns12RZZn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df['full_text'] = news_df[\"news_headline\"].map(str)+ '. ' + news_df[\"news_article\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O2QiDgLwRZZu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f5ae3c0d-4e35-4b67-d4f4-ed356ffd4c79"
      },
      "cell_type": "code",
      "source": [
        "news_df['clean_text'] = normalize_corpus(news_df['full_text'])\n",
        "norm_corpus = list(news_df['clean_text'])\n",
        "news_df.iloc[1][['full_text', 'clean_text']].to_dict()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clean_text': 'kelly rowland say not know excel microsoft excel respond american singer kelly rowland recently appear talk show real say not know microsoft excel not clue kelly ask excel show grammy win song dilemma video feature software respond rowland microsoft excel twitter account tweet no matter crazy',\n",
              " 'full_text': 'Kelly Rowland says she doesn\\'t know Excel, Microsoft Excel responds. American singer Kelly Rowland recently appeared on talk show \\'The Real\\', where she said, \"I don\\'t know what Microsoft Excel is...I don\\'t have a clue.\" Kelly was asked about Excel on the show as her Grammy-winning song Dilemma\\'s video featured the software. Responding to Rowland, Microsoft Excel\\'s Twitter account tweeted, \"No matter what you do, we\\'re crazy over you.\"'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "A9HmoGJFRZZx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the news articles"
      ]
    },
    {
      "metadata": {
        "id": "qdwcRlBqRZZy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df.to_csv('news.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RERpFdEARZZ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tagging Parts of Speech"
      ]
    },
    {
      "metadata": {
        "id": "YUnbn4TKRZZ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df = pd.read_csv('news.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ux2G0PNqRZZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = normalize_corpus(news_df['full_text'], text_lower_case=False, \n",
        "                          text_lemmatization=False, special_char_removal=False)\n",
        "\n",
        "sentence = str(news_df.iloc[1].news_headline)\n",
        "sentence_nlp = nlp(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aJ_eHFpjRZZ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "5a699ee2-763e-4ef1-861f-d9e96961c947"
      },
      "cell_type": "code",
      "source": [
        "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
        "pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>POS tag</th>\n",
              "      <th>Tag type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kelly</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rowland</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>says</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>she</td>\n",
              "      <td>PRP</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>does</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>n't</td>\n",
              "      <td>RB</td>\n",
              "      <td>ADV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>know</td>\n",
              "      <td>VB</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Excel</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Microsoft</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Excel</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>responds</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Word POS tag Tag type\n",
              "0       Kelly     NNP    PROPN\n",
              "1     Rowland     NNP    PROPN\n",
              "2        says     VBZ     VERB\n",
              "3         she     PRP     PRON\n",
              "4        does     VBZ     VERB\n",
              "5         n't      RB      ADV\n",
              "6        know      VB     VERB\n",
              "7       Excel     NNP    PROPN\n",
              "8           ,       ,    PUNCT\n",
              "9   Microsoft     NNP    PROPN\n",
              "10      Excel     NNP    PROPN\n",
              "11   responds     VBZ     VERB"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "gFLPq8DcRZZ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "26d2256e-100e-42d0-971d-b6aae1ffc96f"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "nltk_pos_tagged = nltk.pos_tag(sentence.split())\n",
        "pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>POS tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kelly</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rowland</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>says</td>\n",
              "      <td>VBZ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>she</td>\n",
              "      <td>PRP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>doesn't</td>\n",
              "      <td>VBZ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>know</td>\n",
              "      <td>JJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Excel,</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Microsoft</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Excel</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>responds</td>\n",
              "      <td>NNS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Word POS tag\n",
              "0      Kelly     NNP\n",
              "1    Rowland     NNP\n",
              "2       says     VBZ\n",
              "3        she     PRP\n",
              "4    doesn't     VBZ\n",
              "5       know      JJ\n",
              "6     Excel,     NNP\n",
              "7  Microsoft     NNP\n",
              "8      Excel     NNP\n",
              "9   responds     NNS"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "k9ANGE1qRZaC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Shallow Parsing or Chunking Text"
      ]
    },
    {
      "metadata": {
        "id": "bAvge7mdRZaD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nltk.download('conll2000')\n",
        "\n",
        "from nltk.corpus import conll2000\n",
        "data = conll2000.chunked_sents()\n",
        "\n",
        "train_data = data[:10900]\n",
        "test_data = data[10900:] \n",
        "\n",
        "print(len(train_data), len(test_data))\n",
        "print(train_data[1]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SqpXzInjRZaG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
        "\n",
        "wtc = tree2conlltags(train_data[1])\n",
        "wtc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "88CMAksIRZaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tree = conlltags2tree(wtc) \n",
        "print(tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YZU274h-RZaN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conll_tag_chunks(chunk_sents):\n",
        "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
        "    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
        "\n",
        "\n",
        "def combined_tagger(train_data, taggers, backoff=None):\n",
        "    for tagger in taggers:\n",
        "        backoff = tagger(train_data, backoff=backoff)\n",
        "    return backoff "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ib3Tz_zYRZaQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tag import UnigramTagger, BigramTagger\n",
        "from nltk.chunk import ChunkParserI\n",
        "\n",
        "class NGramTagChunker(ChunkParserI):\n",
        "    \n",
        "  def __init__(self, train_sentences, \n",
        "               tagger_classes=[UnigramTagger, BigramTagger]):\n",
        "    train_sent_tags = conll_tag_chunks(train_sentences)\n",
        "    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
        "\n",
        "  def parse(self, tagged_sentence):\n",
        "    if not tagged_sentence: \n",
        "        return None\n",
        "    pos_tags = [tag for word, tag in tagged_sentence]\n",
        "    chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
        "    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
        "    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
        "                     in zip(tagged_sentence, chunk_tags)]\n",
        "    return conlltags2tree(wpc_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vKoBMc3lRZaS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ntc = NGramTagChunker(train_data)\n",
        "print(ntc.evaluate(test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d_VO6b5LRZaV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "chunk_tree = ntc.parse(nltk_pos_tagged)\n",
        "print(chunk_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mh4Ll1jmRZaY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from IPython.display import display\n",
        "#os.environ['PATH'] = os.environ['PATH']+\";C:\\\\Program Files\\\\gs\\\\gs9.09\\\\bin\\\\\"\n",
        "#display(chunk_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HwnOWlIERZac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Constituency parsing"
      ]
    },
    {
      "metadata": {
        "id": "NJhPlkTmRZad",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set java path\n",
        "import os\n",
        "java_path = r'C:\\Program Files\\Java\\jdk1.8.0_102\\bin\\java.exe'\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "\n",
        "from nltk.parse.stanford import StanfordParser\n",
        "\n",
        "scp = StanfordParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
        "                   path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
        "                   \n",
        "result = list(scp.raw_parse(sentence))\n",
        "print(result[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o_NF1XT0RZag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "os.environ['PATH'] = os.environ['PATH']+\";C:\\\\Program Files\\\\gs\\\\gs9.09\\\\bin\\\\\"\n",
        "display(result[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3W1u9w9CRZaj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dependency parsing"
      ]
    },
    {
      "metadata": {
        "id": "QelU4TWmRZak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dependency_pattern = '{left}<---{word}[{w_type}]--->{right}\\n--------'\n",
        "for token in sentence_nlp:\n",
        "    print(dependency_pattern.format(word=token.orth_, \n",
        "                                  w_type=token.dep_,\n",
        "                                  left=[t.orth_ \n",
        "                                            for t \n",
        "                                            in token.lefts],\n",
        "                                  right=[t.orth_ \n",
        "                                             for t \n",
        "                                             in token.rights]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OommzdamRZan",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(sentence_nlp, jupyter=True, \n",
        "                options={'distance': 110,\n",
        "                         'arrow_stroke': 2,\n",
        "                         'arrow_width': 8})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H29DQ5hsRZax",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.parse.stanford import StanfordDependencyParser\n",
        "sdp = StanfordDependencyParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
        "                               path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')    \n",
        "result = list(sdp.raw_parse(sentence))  \n",
        "dep_tree = [parse.tree() for parse in result][0]\n",
        "print(dep_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cf3pnWrPRZa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "os.environ['PATH'] = os.environ['PATH']+\";C:\\\\Program Files\\\\gs\\\\gs9.09\\\\bin\\\\\"\n",
        "display(dep_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gYXLloMRZa_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from graphviz import Source\n",
        "\n",
        "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
        "source = Source(dep_tree_dot_repr, filename=\"dep_tree\", format=\"png\")\n",
        "source"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PCz7lyfsRZbD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition"
      ]
    },
    {
      "metadata": {
        "id": "NG5ZZu1ORZbD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence = str(news_df.iloc[1].full_text)\n",
        "sentence_nlp = nlp(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ih9foahdRZbF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print([(word, word.ent_type_) for word in sentence_nlp if word.ent_type_])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cX2206aZRZbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "displacy.render(sentence_nlp, style='ent', jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JVMgLoJfRZbT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "named_entities = []\n",
        "for sentence in corpus:\n",
        "    temp_entity_name = ''\n",
        "    temp_named_entity = None\n",
        "    sentence = nlp(sentence)\n",
        "    for word in sentence:\n",
        "        term = word.text \n",
        "        tag = word.ent_type_\n",
        "        if tag:\n",
        "            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
        "            temp_named_entity = (temp_entity_name, tag)\n",
        "        else:\n",
        "            if temp_named_entity:\n",
        "                named_entities.append(temp_named_entity)\n",
        "                temp_entity_name = ''\n",
        "                temp_named_entity = None\n",
        "\n",
        "entity_frame = pd.DataFrame(named_entities, \n",
        "                            columns=['Entity Name', 'Entity Type'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0MNabBGZRZbV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n",
        "                           .size()\n",
        "                           .sort_values(ascending=False)\n",
        "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
        "top_entities.T.iloc[:,:15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eRZ2kX6CRZbZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_entities = (entity_frame.groupby(by=['Entity Type'])\n",
        "                           .size()\n",
        "                           .sort_values(ascending=False)\n",
        "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
        "top_entities.T.iloc[:,:15]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C-BE6EmYRZbc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tag import StanfordNERTagger\n",
        "import os\n",
        "\n",
        "java_path = r'C:\\Program Files\\Java\\jdk1.8.0_102\\bin\\java.exe'\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "\n",
        "sn = StanfordNERTagger('E:/stanford/stanford-ner-2014-08-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
        "                       path_to_jar='E:/stanford/stanford-ner-2014-08-27/stanford-ner.jar')\n",
        "\n",
        "ner_tagged_sentences = [sn.tag(sent.split()) for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2tpoCQdjRZbf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "named_entities = []\n",
        "for sentence in ner_tagged_sentences:\n",
        "    temp_entity_name = ''\n",
        "    temp_named_entity = None\n",
        "    for term, tag in sentence:\n",
        "        if tag != 'O':\n",
        "            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
        "            temp_named_entity = (temp_entity_name, tag)\n",
        "        else:\n",
        "            if temp_named_entity:\n",
        "                named_entities.append(temp_named_entity)\n",
        "                temp_entity_name = ''\n",
        "                temp_named_entity = None\n",
        "\n",
        "#named_entities = list(set(named_entities))\n",
        "entity_frame = pd.DataFrame(named_entities, \n",
        "                            columns=['Entity Name', 'Entity Type'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYZw_W6aRZbg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n",
        "                           .size()\n",
        "                           .sort_values(ascending=False)\n",
        "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
        "top_entities.head(15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "epy1WSwIRZbj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_entities = (entity_frame.groupby(by=['Entity Type'])\n",
        "                           .size()\n",
        "                           .sort_values(ascending=False)\n",
        "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
        "top_entities.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i0ADAWnPRZbl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Emotion and Sentiment Analysis"
      ]
    },
    {
      "metadata": {
        "id": "7phvrTrFRZbl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from afinn import Afinn\n",
        "\n",
        "af = Afinn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xTFoWbXWRZbo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentiment_scores = [af.score(article) for article in corpus]\n",
        "sentiment_category = ['positive' if score > 0 \n",
        "                          else 'negative' if score < 0 \n",
        "                              else 'neutral' \n",
        "                                  for score in sentiment_scores]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EMDVEHkVRZbq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([list(news_df['news_category']), sentiment_scores, sentiment_category]).T\n",
        "df.columns = ['news_category', 'sentiment_score', 'sentiment_category']\n",
        "df['sentiment_score'] = df.sentiment_score.astype('float')\n",
        "df.groupby(by=['news_category']).describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x5O0V0cgRZbt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
        "sp = sns.stripplot(x='news_category', y=\"sentiment_score\", \n",
        "                   hue='news_category', data=df, ax=ax1)\n",
        "bp = sns.boxplot(x='news_category', y=\"sentiment_score\", \n",
        "                 hue='news_category', data=df, palette=\"Set2\", ax=ax2)\n",
        "t = f.suptitle('Visualizing News Sentiment', fontsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ldfjhc-nRZbx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fc = sns.factorplot(x=\"news_category\", hue=\"sentiment_category\", \n",
        "                    data=df, kind=\"count\", \n",
        "                    palette={\"negative\": \"#FE2020\", \n",
        "                             \"positive\": \"#BADD07\", \n",
        "                             \"neutral\": \"#68BFF5\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hoaa9AYLRZb0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pos_idx = df[(df.news_category=='technology') & (df.sentiment_score == 6)].index[0]\n",
        "neg_idx = df[(df.news_category=='technology') & (df.sentiment_score == -15)].index[0]\n",
        "\n",
        "print('Most Negative Tech News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
        "print()\n",
        "print('Most Positive Tech News Article:', news_df.iloc[pos_idx][['news_article']][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wy34vD3aRZb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pos_idx = df[(df.news_category=='world') & (df.sentiment_score == 16)].index[0]\n",
        "neg_idx = df[(df.news_category=='world') & (df.sentiment_score == -12)].index[0]\n",
        "\n",
        "print('Most Negative World News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
        "print()\n",
        "print('Most Positive World News Article:', news_df.iloc[pos_idx][['news_article']][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quE0IcANRZb5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "sentiment_scores_tb = [round(TextBlob(article).sentiment.polarity, 3) for article in news_df['clean_text']]\n",
        "sentiment_category_tb = ['positive' if score > 0 \n",
        "                             else 'negative' if score < 0 \n",
        "                                 else 'neutral' \n",
        "                                     for score in sentiment_scores_tb]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ETDPX8xZRZb-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([list(news_df['news_category']), sentiment_scores_tb, sentiment_category_tb]).T\n",
        "df.columns = ['news_category', 'sentiment_score', 'sentiment_category']\n",
        "df['sentiment_score'] = df.sentiment_score.astype('float')\n",
        "df.groupby(by=['news_category']).describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15CFTHyxRZcB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y6dOv9AzRZcE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fc = sns.factorplot(x=\"news_category\", hue=\"sentiment_category\", \n",
        "                    data=df, kind=\"count\", \n",
        "                    palette={\"negative\": \"#FE2020\", \n",
        "                             \"positive\": \"#BADD07\", \n",
        "                             \"neutral\": \"#68BFF5\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ULlq4yjRZcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pos_idx = df[(df.news_category=='world') & (df.sentiment_score == 0.7)].index[0]\n",
        "neg_idx = df[(df.news_category=='world') & (df.sentiment_score == -0.296)].index[0]\n",
        "\n",
        "print('Most Negative World News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
        "print()\n",
        "print('Most Positive World News Article:', news_df.iloc[pos_idx][['news_article']][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GCN68R_HRZcH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import model_evaluation_utils as meu\n",
        "meu.display_confusion_matrix_pretty(true_labels=sentiment_category, \n",
        "                                    predicted_labels=sentiment_category_tb, \n",
        "                                    classes=['negative', 'neutral', 'positive'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}